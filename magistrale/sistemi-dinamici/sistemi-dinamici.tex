\documentclass[a4paper]{article}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{palatino}
\usepackage{frontespizio}
\usepackage{physics}
\usepackage{mathtools, nccmath}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{empheq}
\usepackage{mathtools}
\usepackage{mathdots}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{caption}
\captionsetup[figure]{labelfont={bf},name={Fig.},labelsep=period}
\usepackage[left=2.5cm, right=2.5cm, top=3cm]{geometry}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}


\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\nouppercase{\leftmark}}
\rhead{\nouppercase{\rightmark}}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.4pt}

\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	hidelinks, 
	colorlinks = true,
	linkcolor = black,
}
\usepackage[thinlines,thiklines]{easybmat}
\usepackage{braket}

\usepackage{cleveref}

\newcommand{\numberset}{\mathbb}
\newcommand{\R}{\numberset{R}}
\newcommand{\C}{\numberset{C}}
\renewcommand{\vec}{\bm}

\theoremstyle{definition}
\newtheorem{thm}{Teorema}[subsection]
\newtheorem{exmp}{Esempio}[section]
\newtheorem{defn}{Definizione}[subsection]
\newtheorem{prop}{Proposizione}[subsection]

\usetikzlibrary{decorations.pathreplacing}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture,baseline=(#1.base)]
	\node (#1) {\strut};}

\tikzstyle{syst} = [rectangle, 
rounded corners, minimum width=2cm, minimum height=2cm,text centered, draw=black]

\newcommand\undermat[2]{%
	\makebox[0pt][l]{$\smash{\underbrace{\phantom{%
					\begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}


\usepackage[OT1]{eulervm}

\begin{document}
	\begin{frontespizio}
		\Universita{Verona}
		\Dipartimento{Informatica}
		\Scuola{Laurea Magistrale in Ingegneria e Scienze Informatiche}
		\Titoletto{}
		\Titolo{Sistemi Dinamici}
		\Sottotitolo{Riassunto dei principali argomenti}
		\Candidato[VR424987]{Danzi Matteo}
		\Annoaccademico{2017/2018}
		\NCandidato{Autore}
	\end{frontespizio}
	
	\tableofcontents
	
	\newpage
	
	\section{Introduzione}
	
	\subsection{Sistema}
		\begin{defn}
			Un sistema è un ente fisico, tipicamente formato da diverse componenti tra
			loro interagenti, che risponde a sollecitazioni esterne producendo un determinato
			comportamento.
		\end{defn}
		
		\begin{exmp}
			Un circuito elettrico costituito da componenti quali resistori, capacitori, induttori, diodi, generatori di corrente e tensione, ecc., costituisce un semplice esempio di sistema dinamico. Il comportamento del sistema può venire descritto dal valore dei segnali di tensione e di corrente nei rami del circuito. Le sollecitazioni che agiscono sul sistema sono le tensioni e le correnti applicate dai generatori, che
			possono essere imposte dall'esterno.
		\end{exmp}
		
	\subsection{Descrizione ingresso-uscita}
		Le grandezze alla base di una descrizione IU sono le \textit{cause esterne} al sistema e gli
		\textit{effetti}. Le cause esterne sono delle grandezze che si generano al di fuori del sistema;
		la loro evoluzione influenza il comportamento del sistema ma non dipende da esso.
		Gli effetti invece sono delle grandezze la cui evoluzione dipende dalle cause esterne
		al sistema e dalla natura del sistema stesso. Di solito si usa la convenzione di definire
		come \textit{ingressi} al sistema le cause esterne, e come \textit{uscite} gli effetti. In generale su un
		sistema possono agire più ingressi così come più di una possono essere le grandezze
		in uscita. \\
		La classica rappresentazione grafica di un sistema per il quale siano stati
		individuati ingressi e uscite è quella mostrata in \hyperref[fig:io]{\textbf{Fig. 1.}} dove può venire
		considerato come un operatore che assegna uno specifico andamento alle grandezze
		in uscita in corrispondenza ad ogni possibile andamento degli ingressi.
		
		
		\begin{figure}[h!]
			\centering
			\begin{tikzpicture}[>=latex]
				\node[syst, rounded corners=false] (a) {S};
				\draw[->] ($ (a.west) +(-.75,.55) $) -- ($ (a.west) +(0,.55) $);
				\draw[->] ($ (a.west) +(-.75,-.55) $) -- ($ (a.west) +(0,-.55) $);
				\draw[->] ($ (a.east) +(0,.55) $) -- ($ (a.east) +(.75,.55) $);
				\draw[->] ($ (a.east) +(0,-.55) $) -- ($ (a.east) +(.75,-.55) $);
				\node (b) at ($ (a.west) +(-1.5,0) $) {$ \begin{matrix}
					u_1(t) \\
					\vdots \\
					u_r(t)
					\end{matrix} $};
				\node (c) at ($ (a.east) +(1.5,0) $) {$ \begin{matrix}
					y_1(t) \\
					\vdots \\
					y_p(t)
					\end{matrix} $};
				
				\node at ($ (a.south) +(0,-.5) $) {sistema}; 
				\node[align=center] at ($ (b.south) +(0,-.5) $) {ingressi\\ (cause)};
				\node[align=center] at ($ (c.south) +(0,-.5) $) {uscite\\ (effetti)};
			\end{tikzpicture}
			\caption{Descrizione in ingresso-uscita }
			\label{fig:io}
		\end{figure}
		
		Di solito su usa la convenzione di indicare con
		\[
			u(t) = \big[\ u_1(t)\ \dots\ u_r(t)\ \big]^T \ \in\ \R^r
		\]
		il vettore degli ingressi e con
		\[
			y(t) = \big[\ y_1(t)\ \dots\ y_p(t)\ \big]^T \ \in\ \R^p
		\]
		il vettore delle uscite.
	
	\subsection{Descrizione in variabili di stato}
		È facile rendersi conto che in generale l'uscita di un sistema in un
		certo istante di tempo non dipende dal solo ingresso al tempo, ma dipende anche
		dall'evoluzione precedente del sistema.
		
		Di questo fatto è possibile tenere conto introducendo una grandezza intermedia tra ingressi e uscite, chiamata \textit{stato} del sistema. Lo stato del sistema gode della	proprietà di concentrare in sè l'informazione sul passato e sul presente del sistema.
		Così come le grandezze di ingresso e uscita, anche lo stato è in generale una
		grandezza vettoriale e viene indicato mediante un vettore di stato
		
		\[
			x(t) = \big[\ x_1(t)\ \dots\ x_n(t)\ \big]^T \ \in\ \R^n
		\]
		dove il numero di componenti del vettore di stato si indica con $ n $ e viene detto \textit{ordine}
		del sistema.
		
		\begin{defn}
			Lo stato di un sistema all'istante di tempo è la grandezza che contiene l'informazione necessaria per determinare univocamente l'andamento dell'uscita $ y(t) $, per ogni $ t \geq t_0 $, sulla base della conoscenza dell'andamento dell'ingresso $ u(t) $, per $ t \geq t_0 $ e appunto dello stato in $ t_0 $.
		\end{defn}
		
		\begin{figure}[t!]
			\centering
			\begin{tikzpicture}[>=latex]
			\node[syst, rounded corners=false, minimum width=3cm, minimum height=2.5cm] (a) {$ \vdots $};
			
			\node[draw, rectangle, minimum width=0.5cm, minimum height=1.5cm] (a1) at ($ (a) +(-1,0) $) {}; 
			\node[draw, rectangle, minimum width=0.5cm, minimum height=1.5cm] (a2) at ($ (a) +(1,0) $) {};
			
			
			\draw[->] ($ (a1.east) +(0,.55) $) -- ($ (a2.west) +(0,.55) $) node[midway, above] {$ x_1(t) $};
			\draw[->] ($ (a1.east) +(0,-.55) $) -- ($ (a2.west) +(0,-.55) $) node[midway, below] {$ x_n(t) $};
			\draw[->] ($ (a.west) +(-.75,.55) $) -- ($ (a1.west) +(0,.55) $);
			\draw[->] ($ (a.west) +(-.75,-.55) $) -- ($ (a1.west) +(0,-.55) $);
			\draw[->] ($ (a2.east) +(0,.55) $) -- ($ (a.east) +(.75,.55) $);
			\draw[->] ($ (a2.east) +(0,-.55) $) -- ($ (a.east) +(.75,-.55) $);
			\node (b) at ($ (a.west) +(-1.5,0) $) {$ \begin{matrix}
				u_1(t) \\
				\vdots \\
				u_r(t)
				\end{matrix} $};
			\node (c) at ($ (a.east) +(1.5,0) $) {$ \begin{matrix}
				y_1(t) \\
				\vdots \\
				y_p(t)
				\end{matrix} $};
			
			\node at ($ (a.south) +(0,-.5) $) {stati}; 
			\node at ($ (b.south) +(0,-.5) $) {ingressi};
			\node at ($ (c.south) +(0,-.5) $) {uscite};
			\end{tikzpicture}
			\caption{Descrizione in variabili di stato }
			\label{fig:state}
		\end{figure}
		
	\section{Modello matematico di un sistema}
		L'obiettivo dell'Analisi dei Sistemi consiste nel studiare il legame esistente tra gli ingressi e le uscite di un sistema e/o tra gli stati, gli ingressi e le uscite del sistema.
		In altri termini, risolvere un problema di analisi significa capire, dati certi segnali in ingresso al sistema, come evolveranno gli stati e le uscite di tale sistema.\\
		Questo rende necessaria la definizione di un modello matematico che descriva in maniera quantitativa il comportamento del sistema allo studio, ossia fornisca una descrizione matematica esatta del legame tra ingressi (stati) e uscite.
		A seconda del tipo di descrizione che si vuole dare al sistema (IU o VS) è necessario formulare due diversi tipi di modello.
		
		\subsection{Modello ingresso-uscita}
			Il modello IU per un sistema MIMO, ossia un sistema con $ p $ uscite e $ r $ ingressi, è espresso mediante $ p $ equazioni differenziali del tipo:
			\[
				\begin{cases}
					h_1 = \left(\underbrace{ y_1(t), \dots, y_1^{(n_1)}(t) }_{\textup{uscita 1}}, 
						\underbrace{ u_1(t), \dots, u_1^{(m_{1,1})}(t) }_{\textup{ingresso 1}}, \dots,
						\underbrace{ u_r(t), \dots, u_r^{(m_{1,r})}(t) }_{\textup{ingresso }r}, t
					\right) = 0\\ \\
					h_2 = \left(\underbrace{ y_2(t), \dots, y_2^{(n_2)}(t) }_{\textup{uscita 2}}, 
						\underbrace{ u_1(t), \dots, u_1^{(m_{2,1})}(t) }_{\textup{ingresso 1}}, \dots,
						\underbrace{ u_r(t), \dots, u_r^{(m_{2,r})}(t) }_{\textup{ingresso }r}, t
					\right) = 0\\
					\qquad \vdots \\
					h_p = \left(\underbrace{ y_p(t), \dots, y_p^{(n_p)}(t) }_{\textup{uscita }p}, 
						\underbrace{ u_1(t), \dots, u_1^{(m_{p,1})}(t) }_{\textup{ingresso 1}}, \dots,
						\underbrace{ u_r(t), \dots, u_r^{(m_{p,r})}(t) }_{\textup{ingresso }r}, t
					\right) = 0\\
				\end{cases}
			\]
			
			dove:
			\begin{itemize}
				\item $ h_i,\ i = 1,\dots, p $ sono funzioni di più parametri che dipendono dal particolare
				sistema allo studio,
				\item $ n_i $ è il grado massimo di derivazione della $ i$-esima componente dell'uscita $ y_i(t) $,
				\item $ m_i $ è il grado massimo di derivazione della $ i$-esima componente dell'ingresso $ u_i(t) $.
			\end{itemize}
			
		\subsection{Modello in variabili di stato}
			Il modello in VS per un sistema MIMO con $ r $ ingressi e $ p $ uscite ha invece una
			struttura del tipo
			\[
				\begin{cases}
					\dot{x}_1 (t) = f_1(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t) \\
					\qquad \quad\vdots \\
					\dot{x}_n (t) = f_n(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t) \\ \\
					y_1(t) = g_1(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t) \\
					\qquad \quad\vdots \\
					y_p(t) = g_p(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t)
				\end{cases}
			\]
			che riscritto in forma matriciale diviene
			\[
				\begin{cases}
					\dot{\vec{x}}(t) = \vec{f}(\vec{x}(t),\vec{u}(t),t) \\
					\vec{y}(t) = \vec{g}(\vec{x}(t),\vec{u}(t),t)
				\end{cases}
			\]
			L'equazione di stato è pertanto un sistema di $ n $ equazioni differenziali del primo
			ordine, a prescindere dal fatto che il sistema sia SISO o MIMO. La \textit{trasformazione
			in uscita} è invece una equazione algebrica, scalare o vettoriale a seconda del numero
			delle variabili in uscita.
			La rappresentazione schematica che si può dare di un modello in VS è pertanto
			quella riportata in \hyperref[fig:VS]{\textbf{Fig. 3.}}
			\begin{figure}[h!]
				\centering
				\begin{tikzpicture}[>=latex]
					\node[draw, rectangle,minimum height=1cm] (a) {$ \dot{\vec{x}}(t) = \vec{f}(\vec{x}(t),\vec{u}(t),t) $};
					\node[draw, rectangle,minimum height=1cm] (b) at (5,0) {$ \vec{y}(t) = \vec{g}(\vec{x}(t),\vec{u}(t),t) $};
					
					\draw[->] ($ (a.west) +(-1,0) $) -- (a.west) node[midway, above] {$ \vec{u}(t) $};
					\draw[->] ($ (a.west) +(-.75,0) $) -- ++(0,-1) -- ++(5,0) -- ++(0,0.75) -- ($ (b.west) +(0,-0.25) $);
					\draw[->] (a.east) -- (b.west) node[midway, above] {$ \vec{x}(t) $};
					\draw[->] (b.east) -- ++(1,0) node[midway, above] {$ \vec{y}(t) $};
				\end{tikzpicture}
				\caption{Rappresentazione schematica di un modello in variabili di stato}
				\label{fig:VS}
			\end{figure}
			
	\section{Proprietà dei sistemi}
		\subsection{Sistemi dinamici o istantanei}
			La prima importante distinzione che si può fare è tra sistemi istantanei e sistemi
			dinamici.
			
			\begin{defn}
				Un sistema è detto
				\begin{itemize}
					\item \textit{Istantaneo}: se il valore $ \vec{y}(t) $ assunto dall'uscita al tempo $ t $ dipende solo dal valore $ \vec{u}(t) $ assunto dall'ingresso al tempo $ t $;
					\item \textit{Dinamico}: in caso contrario.
				\end{itemize}
			\end{defn}
			
			Condizione necessaria e sufficiente affinché un sistema MIMO con $ r $ ingressi e $ p $ uscite sia istantaneo è che il legame \textbf{IU} sia espresso da un insieme di equazioni della forma:
			\[
				\begin{cases}
					h_1 (y_1(t),u_1(t),u_2(t),\dots, u_r(t),t) = 0 \\
					h_2 (y_2(t),u_1(t),u_2(t),\dots, u_r(t),t) = 0 \\
					\vdots \\
					h_p (y_p(t),u_1(t),u_2(t),\dots, u_r(t),t) = 0.
				\end{cases}
			\]
			
			Condizione necessaria e sufficiente affinché un sistema sia istantaneo è che il modello in \textbf{VS} abbia ordine zero ovvero che \textit{non esista il vettore di stato}.
			
		\subsection{Sistemi lineari o non lineari}
			Una delle proprietà fondamentali di cui gode un'ampia classe di sistemi (o più precisamente di modelli) è la linearità. È proprio su questa classe di sistemi che focalizzeremo la nostra attenzione in questo volume. L'importanza dei sistemi lineari deriva
			da una serie di considerazioni pratiche.\\
			La prima è che tali sistemi sono facili da studiare. Per essi sono state proposte
			efficienti tecniche di analisi e di sintesi, non più applicabili se la linearità viene meno.
			In secondo luogo, un modello lineare si rivela una buona approssimazione del
			comportamento di numerosi sistemi reali purchè questi siano sottoposti a piccoli
			ingressi.
			
			\begin{defn}
				Un sistema è detto
				\begin{itemize}
					\item \textit{Lineare}: se per esso vale il principio di sovrapposizione degli effetti. Ciò significa che se il sistema risponde alla causa $ c_1 $ con l'effetto $ e_1 $ e alla causa $ c_2 $ con l'effetto $ e_2 $, allora la risposta del sistema alla causa $ ac_1 + bc_2 $ è $ ae_1 + be_2 $ qualunque siano i valori assunti dalle costanti $ a $ e $ b $.
					Il seguente schema riassume tale proprietà:
					\[
						\begin{rcases*}
							causa\ c_1\ \rightsquigarrow\ effetto\ e_1 \\
							causa\ c_2\ \rightsquigarrow\ effetto\ e_2
						\end{rcases*}
						\
						\Longrightarrow
						\
						causa\ (ac_1 + bc_2)\ \rightsquigarrow\ effetto\ (ae_1 + be_2);
					\]
					\item \textit{Non lineare}: in caso contrario.
				\end{itemize}
			\end{defn}
			
		Condizione necessaria e sufficiente affinché un sistema sia lineare è che il legame \textbf{IU} sia espresso da una equazione differenziale	lineare, cioè per un sistema SISO:
		\[
			a_0(t) y(t) + a_1(t)\dot{y}(t) + \cdots + a_n(t) y^{(n)}(t)\ =\
			b_0(t) u(t) + b_1(t)\dot{u}(t) + \cdots + b_m(t)u^{(m)}(t)
		\]
		Un sistema MIMO invece è lineare se e solo se ciascuna delle funzioni $ h_i,\ i = 1,\dots , p $, esprime una combinazione lineare tra la $ i$-esima componente dell'uscita e le sue $ n_i $ derivate e le variabili di ingresso con le loro derivate.
		
		\bigskip
		
		Condizione necessaria e sufficiente affinché un
		sistema sia lineare è che nel modello in \textbf{VS} sia l'equazione di stato che la trasformazione di uscita siano equazioni lineari:
		\[
			\begin{cases}
				\dot{x}_1(t) = a_{1,1}(t) x_1(t) + \cdots + a_{1,n}(t) x_n(t) + b_{1,1} u_1(t) + \cdots + b_{1,r} u_r(t) \\
				\qquad\vdots \\
				\dot{x}_n(t) = a_{n,1}(t) x_1(t) + \cdots + a_{n,n}(t) x_n(t) + b_{n,1} u_1(t) + \cdots + b_{n,r} u_r(t) \\ \\
				y_1(t) = c_{1,1}(t) x_1(t) + \cdots + c_{1,n}(t) x_n(t) + d_{1,1} u_1(t) + \cdots + d_{1,r} u_r(t) \\
				\qquad\vdots \\
				y_p(t) = c_{p,1}(t) x_1(t) + \cdots + c_{p,n}(t) x_n(t) + d_{p,1} u_1(t) + \cdots + d_{p,r} u_r(t) \\
			\end{cases}
		\]
		ovvero
		\[
			\begin{cases}
				\vec{\dot{x}}(t) = \vec{A}(t)\vec{x}(t) + \vec{B}(t)\vec{u}(t) \\
				\vec{y}(t) = \vec{C}(t)\vec{x}(t) + \vec{D}(t)\vec{u}(t)
			\end{cases}
		\]
		dove
		
		\begin{align*}
			\vec{A}(t) &= \{a_{i,j} (t) \} \ \textup{matrice} \ n\times n;\qquad
			&\vec{B}(t) = \{b_{i,j}(t) \} \ \textup{matrice} \ n\times r; \\
			\vec{C}(t) &= \{c_{i,j} (t) \} \ \textup{matrice} \ p\times n;\qquad
			&\vec{D}(t) = \{d_{i,j}(t) \} \ \textup{matrice} \ p\times r; 
		\end{align*}
		
	\subsection{Tempo-invarianza}
		Un'altra importante proprietà di cui gode un'ampia classe di sistemi è la \textit{tempo-invarianza} o stazionarietà. In particolare, in questo testo ci occuperemo proprio dell'analisi dei sistemi
		lineari e stazionari.
		
		\begin{defn}
			Un sistema è detto
			\begin{itemize}
				\item \textit{Tempo-invariante}: se per esso vale il principio di traslazione causa-effetto nel tempo, cioè se il sistema risponde sempre con lo stesso effetto ad una data causa, a prescindere dall'istante di tempo in cui tale causa agisca
				sul sistema.\\
				Il seguente schema riassume tale proprietà:
				\[
					\textup{causa}\ c(t)\ \rightsquigarrow \ \textup{effetto}\ e(t)\ \
					\Longrightarrow\ \ \textup{causa}\ c(t - T)\  \rightsquigarrow \ \textup{effetto}\ e(t-T);
				\]
				\item \textit{Tempo-variante}: in caso contrario.
			\end{itemize}
			
			La \hyperref[fig:time]{\textbf{Fig. 4.}} mostra il comportamento tipico di un sistema lineare sollecitato
			dalla stessa causa applicata in due diversi intervalli di tempo, ossia a partire da $ t = 0 $
			e a partire da $ t = T $: nei due casi l'effetto risultante è analogo ma ha semplicemente
			origine da istanti di tempo che differiscono tra di loro proprio di una quantità pari a $ T $.
			
			\begin{figure}[h!]
				\centering
				\begin{tikzpicture}[>=latex]
					\draw[->] (0,-.25) -- ++(0,2) node[left] {$ e(t) $}; \draw[->] (-.25,0) -- ++(3,0) node[below] {$ t $};
					\draw[->] (5,-.25) -- ++(0,2) node[left] {$ e(t - T) $}; \draw[->] (4.75,0) -- ++(3,0) node[below] {$ t $};
					\draw[->] (0, 2.75) -- ++(0,2) node[left] {$ c(t) $}; \draw[->] (-.25,3) -- ++(3,0) node[below] {$ t $};
					\draw[->] (5,2.75) -- ++(0,2) node[left] {$ c(t - T) $}; \draw[->] (4.75,3) -- ++(3,0) node[below] {$ t $};
					
					\draw[thick] (-.25,3) -- ++(.25,0) -- ++(0,1) .. controls (0.25,4.7) and (.75,2.5)
					.. (2,4);
					\draw[thick] (4.75,3) -- ++(1,0) -- ++(0,1) .. controls (6 ,4.7) and (6.55,2.5)
					.. (7.75,4);
					
					\draw[thick] (-.25,0) .. controls (1,0) and (1, .75) .. (2, 1.5);
					\draw[thick] (4.75,0) -- (5.5,0);
					\draw[thick,xshift=5.75cm] (-.25,0) .. controls (1,0) and (1, .75) .. (2, 1.5);
					
					\draw[<->] (5,0.25) -- (5.75,0.25) node[midway, above] {$ T $};
					\draw (5.75,0) -- (5.75,.5);
					\draw[<->,yshift=3cm] (5,0.25) -- (5.75,0.25) node[midway, above] {$ T $};
				\end{tikzpicture}
				\caption{Traslazione causa-effetto nel tempo}
				\label{fig:time}
			\end{figure}
		\end{defn}
		
		Condizione necessaria e sufficiente affinché un sistema sia stazionario è che il legame \textbf{IU} non dipenda esplicitamente dal tempo, cioè per un sistema SISO:
		\[
			h \left( y(t), \dot{y}(t), \dots, y^{(n)}(t), u(t), \dot{u}(t),\dots, u^{(m)}(t) \right) = 0
		\]
		che nel caso dei sistemi lineari si riduce a una equazione differenziale lineare a
		coefficienti costanti:
		\[
			a_0 y(t) + a_1\dot{y}(t) + \cdots + a_n y^{(n)}(t) = 
			b_0 u(t) + b_1\dot{u}(t) + \cdots + b_n u^{(m)}(t).
		\]
		
		Condizione necessaria e sufficiente affinché un	sistema sia stazionario è che nel modello in \textbf{VS} l'equazione di stato e la trasformazione di uscita non dipendano esplicitamente dal tempo:
		\[
			\begin{cases}
				\vec{\dot{x}}(t)\ =\vec{f}(\vec{x}(t),\vec{u}(t)) \\
				\vec{y}(t)\ =\vec{g}(\vec{x}(t),\vec{u}(t)) 
			\end{cases}
		\]
		che nel caso dei sistemi lineari si riduce a
		\[
			\begin{cases}
				\vec{\dot{x}}(t)\ =\vec{A}\vec{x}(t)+ \vec{B}\vec{u}(t)  \\
				\vec{y}(t)\ =\vec{C}\vec{x}(t)+ \vec{D}\vec{u}(t) 
			\end{cases}
		\]
		dove $ \vec{A}, \vec{B}, \vec{C} $ e $ \vec{D} $ sono matrici costanti.
		
		\newpage
		
	\section{Analisi nel dominio del tempo delle rappresentazioni VS}
		Un sistema lineare e stazionario di ordine $ n $, con $ r $ ingressi e $ p $ uscite, ha la seguente rappresentazione in VS:
		\begin{equation}\label{eq:sys}
			\begin{cases}
				\vec{\dot{x}}(t)\ =\vec{A}\vec{x}(t)+ \vec{B}\vec{u}(t)  \\
				\vec{y}(t)\ =\vec{C}\vec{x}(t)+ \vec{D}\vec{u}(t) 
			\end{cases}
		\end{equation}
	
		Il problema fondamentale dell'analisi dei sistemi per un tale sistema consiste nel
		determinare l'andamento dello stato $ \vec{x}(t) $ e dell'uscita $ \vec{y}(t) $ per $ t \geq t_0 $ noto:
		\begin{itemize}
			\item il valore dello stato iniziale $ \vec{x}(t_0) $;
			\item l'andamento dell'ingresso $ \vec{u}(t) $ per $ t\geq t_0 $.
		\end{itemize}

		\subsection{Matrice di transizione di stato}
			Data una matrice quadrata $ \vec{A} $ il suo esponenziale è la matrice
			\[
				e^{\vec{A}} = \vec{I} + \vec{A} + \dfrac{\vec{A}^2}{2!} + \dfrac{\vec{A}^2}{3!} + \cdots =
				\sum_{k = 0}^{\infty} \dfrac{\vec{A}^k }{k!}
			\]
			La matrice di transizione dello stato $ e^{\vec{A}} $ è una particolare matrice esponenziale i cui elementi sono funzioni del tempo di dimensione $ n\times n $ e anche $ \vec{A} $ è $ n\times n $
			\[
				e^{\vec{A}t} = \sum_{k=0}^{\infty} \dfrac{\vec{A}^k t^k}{k!}
			\]
			Se $ \vec{A} $ è una matrice \textbf{diagonale} di dimensione $ n\times n $
			\[
				\vec{A} = 
				\begin{bmatrix}
					\lambda_1 & 0 & \cdots & 0 \\
					0 & \lambda_2 & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & \lambda_n 
				\end{bmatrix}
				\qquad
				\textup{vale}
				\qquad
				e^{\vec{A}t} =
				\begin{bmatrix}
					e^{\lambda_1 t} & 0 & \cdots & 0 \\
					0 & e^{\lambda_2 t} & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & e^{\lambda_n t}
				\end{bmatrix}
			\]
			
			
		\subsection{Sviluppo di Sylvester}
			Ci si pone ora il problema di determinare l'espressione analitica della matrice di transizione dello stato $ e^{\vec{A}t} $ senza dover necessariamente calcolare la serie infinita che la
			definisce. Tale procedura si basa sullo sviluppo di Sylvester, esiste una seconda procedura, basata sul passaggio alla forma diagonale o alla forma di Jordan e una terza procedura, basata sull'uso delle
			trasformate di Laplace.
			
			\begin{defn}
				Se $ \vec{A} $ è una matrice di dimensione $ n\times n $, la corrispondente matrice di transizione dello stato $ e^{\vec{A}t} $ può essere scritta come:
				\[
					e^{\vec{A}t} = \sum_{i = 0}^{n - 1} \beta_i (t)\vec{A}^i = 
					\beta_0 (t)\vec{I} + \beta_1(t)\vec{A} + \cdots + \beta_{n-1}(t)\vec{A}^{n-1}
				\]
				dove i coefficienti dello sviluppo $ \beta_i(t) $ sono opportune funzioni scalari nel tempo.			
			\end{defn}
			I coefficienti dello sviluppo di Sylvester possono venire determinati risolvendo un
			sistema di equazioni lineari. Esistono sostanzialmente tre diversi casi che vengono discussi di seguito.
			
		\subsubsection{Autovalori di molteplicità unitaria}
			Se la matrice $ \vec{A} $ ha autovalori tutti distinti $ \lambda_1, \lambda_2,\dots, \lambda_n $, le $ n $ funzioni incognite $ \beta_i(t) $ si ricavano risolvendo il seguente sistema di $ n $ equazioni (tante equazioni quanti sono gli autovalori):
			\[
				\begin{cases}
					\beta_0 (t) + \lambda_1\beta_1 (t) + \lambda_1^2\beta_2 (t) + \cdots + \lambda_1^{n-1} \beta_{n-1}(t) = e^{\lambda_1 t} \\
					\beta_0 (t) + \lambda_2\beta_1 (t) + \lambda_2^2\beta_2 (t) + \cdots + \lambda_2^{n-1} \beta_{n-1}(t) = e^{\lambda_2 t} \\
					\qquad \vdots \qquad \vdots \\
					\beta_0 (t) + \lambda_n\beta_1 (t) + \lambda_n^2\beta_2 (t) + \cdots + \lambda_n^{n-1} \beta_{n-1}(t) = e^{\lambda_n t} \\
				\end{cases}
			\]
			ovvero risolvendo il sistema di equazioni lineari
			\[
				\vec{V}\vec{\beta} = \vec{\eta}
			\]
			dove
			\begin{itemize}
				\item $ \beta = \begin{bmatrix}
					\beta_0(t) & \beta_1(t) & \cdots& \beta_{n-1}(t)
				\end{bmatrix}^T $ è il vettore delle incognite
				\item La matrice dei coefficienti vale
				\[
					\vec{V} = 
					\begin{bmatrix}
						1 & \lambda_1 & \cdots & \lambda_1^{n-1} \\
						1 & \lambda_2 & \cdots & \lambda_2^{n-1} \\
						\vdots & \vdots & \ddots & \vdots \\
						1 & \lambda_n & \cdots & \lambda_n^{n-1} \\
					\end{bmatrix}
				\]
				\item $ \eta = \begin{bmatrix}
				e^{\lambda_1 t} & e^{\lambda_2 t} & \cdots & e^{\lambda_n t}
				\end{bmatrix}^T $ è il vettore dei termini noti.
			\end{itemize}
		
			Viene detta \textbf{modo della matrice} $ \vec{A} $ associato all'autovalore $ \lambda $ la generica componente $ e^{\lambda t} $ (una funzione nel tempo) del vettore  $ \vec{\eta} $.
			
		\subsubsection{Autovalori di molteplicità non unitaria}
			Se la matrice $ \vec{A} $ ha autovalori di molteplicità non unitaria, si costruisce un sistema in cui ad ogni autovalore $ \lambda $ di molteplicità $ \nu $ corrispondono $ \nu $ equazioni della forma:
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1} \beta_{n-1} (t) &= e^{\lambda t} \\
				\dfrac{d}{d\lambda}\left( \beta_0(t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1}\beta_{n-1}(t) \right) &= \dfrac{d}{d\lambda}e^{\lambda t} \\
				\qquad \vdots \qquad \qquad \qquad \qquad & \qquad \vdots \\
				\dfrac{d^{\nu -1}}{d\lambda^{\nu -1}}\left( \beta_0(t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1}\beta_{n-1}(t) \right) &= \dfrac{d^{\nu -1}}{d\lambda^{\nu -1}}e^{\lambda t} \\
			\end{empheq}
			ovvero
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1} \beta_{n-1} (t) &= e^{\lambda t} \\
				\beta_1(t) + 2\lambda\beta_2 (t) + \cdots + (n - 1)\lambda^{n-2}\beta_{n-1}(t) &= t e^{\lambda t} \\
				\qquad \vdots \qquad \qquad \qquad \qquad & \qquad \vdots \\
				\dfrac{(\nu -1)!}{0!} \beta_{\nu - 1}(t) + \cdots + 
				\dfrac{(n-1)!}{(n - \nu)!}\lambda^{n-\nu}\beta_{n-1}(t) &= t^{\nu - 1} e^{\lambda t} .\\
			\end{empheq}
			
			Anche in tal caso è possibile scrivere un sistema lineare in cui ad ogni autovalore $ \lambda $ i molteplicità $ \nu $ sono associate $ \nu $ righe della matrice dei coefficienti $ \vec{V} $:
			\[
				\begin{bmatrix}
					1 & \lambda & \lambda^2 & \cdots & \lambda^{\nu - 1} & \cdots & \lambda^{n - 1} \\
					0 & 1 & 2\lambda & \cdots & (\nu - 1)\lambda^{\nu - 2} & \cdots & (n - 1)\lambda^{n-2} \\
					\vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
					0 & 0 & 0 & \cdots & (\nu - 1)! & \cdots & \dfrac{(n-1)!}{(n - \nu)!}\lambda^{n - \nu}
				\end{bmatrix}
			\]
			e $ \nu $ righe del vettore dei termini noti $ \vec{\eta} = 
			\begin{bmatrix}
				e^{\lambda t} & te^{\lambda t} & \cdots & t^{\nu - 1}e^{\lambda t}
			\end{bmatrix}^T $.
			
		\subsubsection{Autovalori complessi}
			Anche nel caso in cui vi siano autovalori complessi è possibile determinare i
			coefficienti dello sviluppo di Sylvester come sopra indicato.\\
			Per evitare, tuttavia, di lavorare con numeri complessi conviene modificare la
			procedura per il calcolo dei coefficienti $ \beta $ come segue (tratteremo solo il caso di autovalori di molteplicità unitaria per semplicità). Supponiamo che fra gli $ n $ autovalori della matrice ve ne siano 2 complessi e coniugati $ \lambda, \lambda' = \alpha \pm j \omega $.
			
			In tal caso nel sistema di equazioni dovrebbero comparire le due equazioni
			
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + \lambda_1\beta_1 (t) + \lambda_1^2\beta_2 (t) + \cdots & + \lambda_1^{n-1} \beta_{n-1}(t) &= e^{\lambda_1 t} &= e^{\alpha t}e^{j\omega t}\\
				\beta_0 (t) + \lambda'\beta_1 (t) + (\lambda' )^2\beta_2 (t) + \cdots & + (\lambda')^{n-1} \beta_{n-1}(t) &= e^{\lambda' t} &=e^{\alpha t}e^{j\omega t}
			\end{empheq}
			Possiamo tuttavia sostituire queste due equazioni con due equazioni equivalenti
			in cui non compaiono termini complessi:
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + Re(\lambda)\beta_1 (t) + Re(\lambda^2)\beta_2 (t) + \cdots & + Re(\lambda^{n-1}) \beta_{n-1}(t) &= e^{\lambda_1 t} cos(\omega t)\\
				Im(\lambda)\beta_1 (t) + Im(\lambda )^2\beta_2 (t) + \cdots & + Im(\lambda^{n-1}) \beta_{n-1}(t) &= e^{\lambda t} sin(\omega t)
			\end{empheq}
			dove $ Re $ e $ Im $ indicano la parte reale e immaginaria di un numero complesso. In particolare dunque vale $ Re(\lambda) = \alpha $ e $ Im(\lambda) = \omega $
			
			
	\subsection{Formula di Lagrange}
		Possiamo finalmente dimostrare un importante risultato che determina la soluzione
		al problema di analisi per i sistemi MIMO precedentemente enunciato. Tale risultato
		è noto con il nome di formula di Lagrange.
		\begin{defn}
			La soluzione del sistema 
			\[
			\begin{cases}
			\vec{\dot{x}}(t)\ =\vec{A}\vec{x}(t)+ \vec{B}\vec{u}(t)  \\
			\vec{y}(t)\ =\vec{C}\vec{x}(t)+ \vec{D}\vec{u}(t) 
			\end{cases}
			\]
			con stato iniziale $ \vec{x}(t_0) $ e andamento dell'ingresso $ \vec{u}(t) $ (per $ t\geq t_0 $) vale per $ t\geq t_0 $:
			\[
				\begin{cases}
					\vec{x}(t) = e^{\vec{A}(t - t_0)}\vec{x}(t_0) + 
					\displaystyle\int_{t_0}^{t} e^{\vec{A}(t-\tau)}\vec{B}\vec{u}(\tau)d\tau \\ \\
					\vec{y}(t) = \vec{C}e^{\vec{A}(t - t_0)}\vec{x}(t_0) + 
					\vec{C}\displaystyle\int_{t_0}^{t} \vec{B}\vec{u}(\tau)d\tau + \vec{D}\vec{u}(t)
				\end{cases}
			\]
		\end{defn}
		
		
		\subsubsection{Evoluzione libera e evoluzione forzata}
			In base al precedente risultato possiamo anche scrivere l'\textbf{evoluzione dello stato} per $ t\geq t_0 $
			come la somme di due termini:
			\[
				\vec{x}(t) = \vec{x}_l(t) + \vec{x}_f(t).
			\]
			\begin{itemize}
				\item Il termine 
				\[
					\vec{x}_l(t) = e^{\vec{A}(t-t_0)}\vec{x}(t_0)
				\]
				corrisponde all'\textit{evoluzione libera dello stato} a partire dalle condizioni iniziali $ \vec{x}(t_0) $. Si noti che $ e^{\vec{A}(t-t_0)} $ indica appunto come avviene la transizione dallo stato $ \vec{x}(t_0) $ allo stato $ \vec{x}(t) $ in assenza di contributi dovuti all'ingresso.
				\item Il termine
				\[
					\vec{x}_f(t) = \int_{t_0}^{t} e^{\vec{A}(t - \tau)}\vec{B}\vec{u}(\tau)d\tau =
					\int_{0}^{t-t_0} e^{\vec{A}\tau}\vec{B}\vec{u}(t - \tau)d\tau
				\]
				corrisponde all'\textit{evoluzione forzata dello stato} (la seconda equazione si dimostra
				per cambiamento di variabile) . Si osservi che in tale integrale il contributo di 
				$ \vec{u}(\tau) $ allo stato $ \vec{x}(t) $ è pesato tramite la funzione ponderatrice 
				$ e^{\vec{A}(t - \tau)}\vec{B} $.
			\end{itemize}
			
			Anche l'\textbf{evoluzione dell'uscita} per $ t\geq t_0 $ si può scrivere come la somma di due termini:
			\[
				\vec{y}(t) = \vec{y}_l(t) + \vec{y}_f(t).
			\]
			\begin{itemize}
				\item Il termine
				\[
					\vec{y}_l(t) = \vec{C}\vec{x}_l(t) = \vec{C}e^{\vec{A}(t-t_0)}\vec{x}(t_0)
				\]
				corrisponde all'\textit{evoluzione libera dell'uscita} a partire dalle condizioni iniziali $ \vec{y}(t_0) = \vec{C}\vec{x}(t_0) $.
				\item Il termine
				\[
					\vec{y}_f(t) = \vec{C}\vec{x}_f(t) + \vec{D}\vec{u}(t) = 
					\vec{C}\int_{t_0}^{t} \vec{B}\vec{u}(\tau)d\tau + \vec{D}\vec{u}(t)
				\]
				corrisponde all'\textit{evoluzione forzata dell'uscita}.
			\end{itemize}
		
		\subsubsection{Risposta impulsiva di una rappresentazione in VS}
			La risposta impulsiva $ \omega(t) $ di un sistema SISO è la risposta forzata che consegue all’applicazione di un impulso unitario e dunque, posto $ u(t) = \delta(t) $, in base alla formula di Lagrange vale:
			\[
				\omega(t) = \vec{C}\int_{0}^{t} e^{\vec{A}t}\vec{B}\delta(t - \tau)d\tau + D\tau (t).
			\]
			Ricordando la fondamentale proprietà della funzione di Dirac per cui se $ f $ è una funzione continua in $ \left[ t_a,t_b \right] $ e $ t $ appartiene a questo intervallo vale
			\[
				\int_{t_a}^{t_b} f(\tau)\delta(\tau - t) d\tau =
				\int_{t_a}^{t_b} f(\tau)\delta(t - \tau) d\tau = f(t)
			\]
			si ottiene finalmente
			\[
				\omega(t) = \vec{C}e^{\vec{A}t}\vec{B} + D\delta(t)
			\]
			
		\subsection{Trasformazione di similitudine}
			La forma assunta da una rappresentazione in VS di un dato sistema dipende dalla
			scelta delle grandezze che si considerano come variabili di stato. Tale scelta non è
			unica e infatti si possono dare infinite diverse rappresentazioni dello stesso sistema,
			tutte legate da un particolare tipo di trasformazione detta di similitudine.\\
			Uno dei principali vantaggi di questa procedura consiste nel fatto che attraverso
			particolari trasformazioni è possibile passare a nuove rappresentazioni in cui la matrice di stato assume una \textit{forma canonica} particolarmente facile da studiare.\\
			Esempi di forme canoniche sono la \textit{forma diagonale} e la \textit{forma di Jordan}.
			
			\begin{defn}
				Data una rappresentazione della forma~\eqref{eq:sys} si consideri il vettore $ \vec{z}(t) $ legato a $ \vec{x}(t) $ dalla trasformazione
				\[
					\vec{x}(t) = \vec{P}\vec{z}(t)
				\]
				dove $ \vec{P} $ è una qualunque matrice di costanti $ n\times n $ non-singolare. Dunque esiste sempre l'inversa di $ \vec{P} $ e vale anche $ \vec{z}(t) = \vec{P}^{-1}\vec{x}(t) $. Tale trasformazione è la \textbf{trasformazione di similitudine} e la matrice $ \vec{P} $ è detta \textit{matrice di similitudine}.
			\end{defn}
			La trasformazione di similitudine porta ad una nuova rappresentazione.
			\[
				\begin{cases}
					\vec{\dot{z}}(t) = \vec{A'}\vec{z}(t) + \vec{B'}\vec{u}(t) \\
					\vec{y}(t) = \vec{C'}\vec{z}(t) + \vec{D'}\vec{u}(t)
				\end{cases}
			\]
			dove
			\begin{align*}
				\vec{A'} &= \vec{P}^{-1}\vec{A}\vec{P}; &\vec{B'} = \vec{P}^{-1}\vec{B};\\
				\vec{C'} &= \vec{C}\vec{P}; &\vec{D'} = \vec{D}.
			\end{align*}
			 
		\subsection{Diagonalizzazione}
			Si considera adesso il caso di una particolare trasformazione di similitudine che,
			sotto opportune ipotesi, permette di passare ad una matrice $ \vec{\Lambda} = \vec{P}^{-1}\vec{A}\vec{P} $ in forma diagonale.
			
			Una rappresentazione in cui la matrice di stato è in forma diagonale è detta \textit{forma canonica diagonale} ed essa si presta ad una semplice interpretazione fisica. Si consideri ad esempio un sistema SISO la cui equazione vale:
			\[
				\begin{bmatrix}
					\dot{x}_1 (t) \\
					\dot{x}_2 (t) \\
					\vdots \\
					\dot{x}_n (t)
				\end{bmatrix}
				=
				\begin{bmatrix}
					\lambda_1 & 0 & \cdots & 0 \\
					0 & \lambda_2 & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & \lambda_n
				\end{bmatrix}
				\begin{bmatrix}
					x_1 (t) \\
					x_2 (t) \\
					\vdots \\
					x_n (t)
				\end{bmatrix}
				+
				\begin{bmatrix}
					b_1 \\
					b_2 \\
					\vdots\\
					b_n
				\end{bmatrix}
				u(t)
			\]
			L’evoluzione della $ i$-esima componente dello stato è retta dall’equazione
			\[
				\dot{x}_i(t) = \lambda_i x_i(t) + b_i u(t)
			\]
			
			dalla quale si vede che la derivata della componente $ i$-esima non è influenzata dal valore delle altre componenti.
			
			Possiamo dunque pensare a questo sistema come ad una collezione di sottosistemi di ordine 1, ciascuno descritto da una componente del vettore di stato, che evolvono indipendentemente. Il sistema corrispondente alla componente $ i$-esima ha polinomio caratteristico $ P_i = (s-\lambda_i) $ e ad esso corrisponde il modo $ e^{\lambda_i t} $. Talvolta si è anche soliti definire una rappresentazione diagonale con il termine \textit{disaccoppiata} per indicare appunto 'indipendenza fra i diversi nodi.
			
			Il passaggio da una rappresentazione generica ad una rappresentazione in forma
			diagonale richiede una particolare matrice di similitudine.
			
			\begin{defn}
				Data una matrice $ \vec{A} $ di dimensione $ n\times n $ siano $ \vec{v}_1, \vec{v}_2, \dots, \vec{v}_n $ un insieme di autovettori linearmente indipendenti corrispondenti agli autovalori $ \lambda_1,\lambda_2,\dots,\lambda_n $. Definiamo \textit{matrice modale} di $ \vec{A} $ la matrice $ n\times n $
				\[
					\vec{V} = \left[\ \vec{v}_1\ \big|\ \vec{v}_2\ \big|\ \cdots\ \big|\ \vec{v}_n\ \right]
				\]
			\end{defn}
			
			Ogni matrice che ammette matrice modale è \textit{diagonalizzabile}:
			
			\begin{prop}
				Data una matrice $ \vec{A} $ di dimensione $ n\times n $ e autovalori $ \lambda_1,\lambda_2,\dots,\lambda_n $ e $ \vec{V} $ una sua matrice modale. La matrice $ \vec{\Lambda} $ ottenuta attraverso la trasformazione di similitudine
				\[
					\vec{\Lambda} = \vec{P}^{-1}\vec{A}\vec{P}
				\]
				è \textit{diagonale}.
			\end{prop}
			
		\subsection{Forma di Jordan}
			Si consideri una matrice $ \vec{A} $ di dimensione $ n\times n $ i cui autovalori hanno molteplicità
			non unitaria. In tal caso non vi è garanzia che esistano $ n $ autovettori linearmente indipendenti con cui costruire una matrice modale: dunque non è sempre possibile determinare una trasformazione di similitudine che porti ad una forma diagonale.\\
			Si dimostra, tuttavia, che è sempre possibile, estendendo il concetto di autovettore, determinare un insieme di $ n $ \textit{autovettori generalizzati} linearmente indipendenti.\\
			Tali vettori possono venire usati per costruire una \textit{matrice modale generalizzata} che 
			consente, per similitudine, di passare ad una matrice in \textit{forma di Jordan}, una forma
			canonica diagonale a blocchi che generalizza la forma diagonale.
			
			\begin{defn}
				Dato un numero complesso $ \lambda \in \C $ e un numero intero $ p\geq 1 $ definiamo \textbf{blocco di Jordan} di ordine $ p $ associato a $ \lambda $ la matrice quadrata
				\[
					\vec{J} =
					\left.
					\begin{bmatrix}
						\lambda & 1 & 0 & \cdots & 0 & 0 \\
						0 & \lambda & 1 & \cdots & 0 & 0 \\
						0 & 0 & \lambda & \cdots & 0 & 0 \\
						\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
						0 & 0 & 0 & \cdots & \lambda & 1 \\
	 					\tikzmark{lower1}0 & 0 & 0 & \cdots & 0 & \lambda\tikzmark{lower2}
					\end{bmatrix}
					\quad
					\right\rbrace 
					\text{p}
				\]
				\begin{tikzpicture}[overlay, remember picture,decoration={brace,amplitude=5pt}]
				\draw[decorate,thick] ($ (lower2.south) +(0,-.25) $) -- ($ (lower1.south) +(0,-.25) $)
				node [midway,below=5pt] {p};
				\end{tikzpicture}
				
				\bigskip\bigskip
				
				Ogni elemento lungo la diagonale di tale matrice vale $ \lambda $, mentre ogni elemento lugo la sopra diagonale vale $ 1 $; ogni altro elemento è nullo. Dunque $ \lambda $ è un autovalore di molteplicità algebrica $ p $ del blocco $ \vec{J} $.
			\end{defn}
			Possiamo ora definire la forma canonica di Jordan.
			\begin{defn}(Forma di Jordan)
				Una matrice $ \vec{A} $ è detta in \textbf{forma di Jordan} se essa è una matrice diagonale a blocchi
				\[
					\vec{A} =
					\begin{bmatrix}
						\vec{J}_1 & \vec{0} & \cdots & \vec{0} \\
						\vec{0} & \vec{J}_2 & \cdots & \vec{0} \\
						\vdots & \vdots & \ddots & \vdots \\
						\vec{0} & \vec{0} & \cdots & \vec{J}_q
 					\end{bmatrix}
				\]
				In cui ogni blocco $ \vec{J}_i $ lungo la diagonale è un blocco di Jordan associato ad un autovalore $ \lambda_i $ (per $ i = 1,\dots,q $).
			
			
			\textbf{Matrice di Jordan:}
			\[
				\vec{J} = 
				\begingroup % keep the change local
				\setlength\arraycolsep{10pt}
				\begin{bmatrix}
					\boxed{\tikzmark{lower4}\vec{J}_1} & \vec{0} & \cdots & \vec{0} \\[.5em]
					\vec{0} & \boxed{\vec{J}_2} & \cdots & \vec{0} \\[.5em]
					\vdots & \vdots & \ddots & \vdots \\[.5em]
					\vec{0} & \vec{0} & \cdots & \boxed{\vec{J}_q\tikzmark{lower3}} \\[.5em]
				\end{bmatrix}
				\endgroup
			\]
			\begin{tikzpicture}[overlay, remember picture,decoration={brace,amplitude=3pt}]
			\draw[decorate,thick] ($ (lower3.north) +(.65,0) $)  -- ($ (lower3.south) +(.65,0) $)
			node [midway,right=5pt] {$ \mu_{a_q} $};
			\draw[decorate,thick] ($ (lower4.west) +(-.1,.5) $)  -- ($ (lower4.east) +(.5,.5) $)
			node [midway,above=5pt] {$ \mu_{a_1} $};
			\end{tikzpicture}
			
			\textbf{Blocco di Jordan:}
			\[
				\vec{J}_i =
				\begin{bmatrix}
					\boxed{\vec{J}_{1,1}} & \vec{0} & \cdots & \vec{0}\tikzmark{lower5} \\[.5em]
					\vec{0} & \boxed{\vec{J}_{2,2}} & \cdots & \vec{0} \\[.5em]
					\vdots & \vdots & \ddots & \vdots \\[.5em]
					\vec{0} & \vec{0} & \cdots & \boxed{\vec{J}_{1,q_i}\tikzmark{lower6}} \\[.5em]
				\end{bmatrix}
			\]
			\begin{tikzpicture}[overlay, remember picture,decoration={brace,amplitude=5pt}]
			\draw[decorate,thick] ($ (lower5.north) +(1,0) $)  -- ($ (lower6.south) +(.75,0) $)
			node [midway,right=5pt] {$ \mu_{a_i} $};
			\end{tikzpicture}
			
			
			\textbf{Mini blocco di Jordan:}
			\[
				\vec{J}_{i,j} =
				\begin{bmatrix}
					\lambda_1 & 1 & 0 & \cdots & 0 & 0 \\
					0 & \lambda_2 & 1 & \cdots & 0 & 0 \\
					0 & 0 & \lambda_3 & \cdots & 0 & 0 \\
					\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
					0 & 0 & 0 & \cdots & \lambda_{i-1} & 1 \\
					0 & 0 & 0 & \cdots & 0 & \lambda_i
				\end{bmatrix}
			\]
			dove
			\begin{itemize}
				\item $ q $ è il numero di autovalori $ \lambda_i $ distinti
				\item $ i\ \rightarrow\ \lambda_i $
				\item $ j\ \rightarrow\ i,\dots, q_i $
				\item $ \mu_{a_i} $ è la dimensione del blocco di Jordan $ \vec{J}_i $ ed è la molteplicità algebrica che coincide con il rango di $ \vec{J}_i $ .
				\item $ \mu_{g_i} $ è il numero di mini blocchi $ \vec{J}_{i,j} $ ed è la molteplicità geometrica dell'autovalore $ \lambda_i $
				\item $ 1 \leq \mu_g \leq \mu_a \leq n $
			\end{itemize}
		\end{defn}
			
			\begin{defn}[Struttura di autovettori generalizzati]
				Sia $ \vec{A} $ una matrice $ n\times n $ e sia $ \lambda $ un autovalore di molteplicità $ \nu $ a cui corrispondono $ \mu $ autovettori linearmente indipendenti ( con $ 1\leq \mu \leq \nu $).
				A tale autovalore compete una struttura di $ \nu $ autovettori generalizzati linearmente indipendenti costituita da $ \mu $ catene:
				\[
					\begin{cases}
						\vec{v}_{p_1}^{(1)}\ \rightarrow\ \cdots \ \rightarrow\ \vec{v}_{2}^{(1)}\ \rightarrow\ \vec{v}_{1}^{(1)} \qquad & \textup{catena 1} \\ \\
						\vec{v}_{p_2}^{(2)}\ \rightarrow\ \cdots \ \rightarrow\ \vec{v}_{2}^{(2)}\ \rightarrow\ \vec{v}_{1}^{(2)} \qquad & \textup{catena 2} \\
						\qquad\qquad \vdots & \qquad \vdots \\
						\vec{v}_{p_\mu}^{(\mu)}\ \rightarrow\ \cdots \ \rightarrow\ \vec{v}_{2}^{(\mu)}\ \rightarrow\ \vec{v}_{1}^{(\mu)} \qquad & \textup{catena }\mu 
					\end{cases}
				\]
				il numero di catene $ \mu $ è detto \textbf{molteplicità geometrica} dell'autovalore $ \lambda $.
				La $ i-$esima catena ha lunghezza $ p_i $ e termina con un autovettore $ \vec{v}_1^{(i)} $. Gli altri autovettori della catena sono autovettori generalizzati ma non sono autovettori. Poiché in totale gli autovettori generalizzati sono $ \nu $ vale anche 
				\[
					\sum_{i=1}^{\mu} p_i = \nu
				\]
				La lunghezza della catena più lunga $ \pi = \max \{p_1,p_2,\dots,p_{\mu}\} $ è detta \textbf{indice} dell'autovalore $ \lambda $.
 			\end{defn}
 			
 			\subsubsection{Teorema di Cayley-Hamilton}
	 			Il seguente teorema di Cayley-Hamilton definisce il concetto di funzione polinomiale di una matrice quadrata e afferma che \textit{una matrice è radice del proprio polinomio caratteristico}.
	 			
	 			\begin{thm}[Cayley-Hamilton]
					Data una matrice quadrata $ \vec{A} $ di ordine $ n $, sia
					\[
						P(s) = s^n + a_{n-1}s^{n-1} + \cdots + a_1 s + a_0
					\]
					il suo \textbf{polinomio caratteristico}. La matrice $ \vec{A} $ è \textbf{radice} del suo stesso polinomio caratteristico, ovvero soddisfa la seguente equazione:
					\[
						P(\vec{A}) \triangleq \vec{A}^n + a_{n-1} \vec{A}^{n-1} + \cdots + \vec{A}_1 s + a_0\vec{I} = \vec{0}
					\]
	 			\end{thm}
	 			
	 			dove $ \vec{0} $ è una matrice quadrata di ordine $ n $ i cui elementi valgono tutti zero.
 			
 			\subsubsection{Condizioni di diagonalizzabilità}
	 			La matrice $ \vec{A}\ n\times n $ è diagonalizzabile se e solo se è verificata una delle seguenti condizioni:
	 			\begin{itemize}
	 				\item Esistono $ n $ autovettori linearmente indipendenti.
	 				\item Se $ \mu_{a_i} $ dell'autovalore $ \lambda_i $ è uguale alla sua molteplicità geometrica $ \mu_{g_i} $.
	 				\item Se la dimensione di tutti i mini blocchi di Jordan associato a $ \lambda_i $ è unitaria.
	 				\item Se per ogni $ \lambda_i $ la dimensione $ m_i $ del più grande mini blocco di Jordan è unitaria.
	 				\item Se il grado di molteplicità di ogni $ \lambda_i $ nel polinomio minimo $ m(\lambda) $ è unitario.
	 			\end{itemize}
		
		\section{Jordan e cambiamento di base tutorial}
			Data una matrice $ \vec{A} $ per ricavare la matrice $ \vec{T} $ di cambiamento di base (matrice modale) e poi la matrice di Jordan $ \vec{J} $ si eseguono i seguenti passi:
			\begin{enumerate}
				\item Calcolo del polinomio caratteristico $ P_A = \text{det}(\vec{A} - \lambda \vec{I}) $. Da questo mi ricavo gli autovalori $ \lambda_n $ e la molteplicità algebrica $ \mu_a $.
				\item Per ogni autovalore $ \lambda_i,\ i= 1\dots n\ $ calcolo lo spazio nullo $ N(\vec{A} - \lambda_i\vec{I}) $ e ricavo quindi la dimensione dello spazio nullo $ \text{dim}(N) $.
				\item Se $\ \text{dim}(N) < \mu_a\ $ allora calcolo $ N(\vec{A} - \lambda_i\vec{I})^p $ finché $\ \text{dim}(N) = \mu_a\ $.
				\item Una volta trovato $ p $, scelgo un vettore $ v $ che non è presente nello spazio nullo precedente.
				\item Calcolo così la catena di Jordan a ritroso ricavando gli altri vettori: 
				$\ \vec{v}_{n-1} = (\vec{A} - \lambda_n)\vec{v}_n $.
				\item Ottengo così la matrice di trasformazione $ \vec{T} $ fatta in questo modo:
				\[
					\vec{T} =
					\begin{bmatrix}
						\vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_n
					\end{bmatrix}
				\]
			\end{enumerate}
			
			\begin{exmp}
				Data la matrice 
				\[
					\vec{A} = 
					\begin{bmatrix}
						2 & 0 & -1 & 1 \\
						1 & 1 & -1 & 1 \\
						1 & -3 & 2 & -1 \\
						0 & -3 & 2 & -1
					\end{bmatrix}
				\] 
				calcolare la matrice  $ \vec{T} $ di cambiamento di base e poi la $ \vec{A} $ in forma di Jordan data dalla formula 
				\[
					\vec{A} = \vec{T}^{-1} \vec{J} \vec{T}
				\]
				
				\begin{enumerate}
					\item Calcolo il polinomio caratteristico $ P_A $:
					\begin{align*}
						&\text{det}(\vec{A} - \lambda \vec{I}) = 
						\text{det}
						\begin{bmatrix}
							2-\lambda & 0 & -1 & 1\\
							1 & 1-\lambda & -1 & 1 \\
							1 & -3 & 2 - \lambda & -1\\
							0 & -3 & 2 & -1-\lambda
						\end{bmatrix} \\ \\
						&= (2-\lambda)\cdot\text{det}
						\begin{bmatrix}
							1-\lambda & -1 & 1\\
							-3 & 2-\lambda & -1\\
							-3 & 2 & -1-\lambda
						\end{bmatrix}
						-1\cdot\text{det}
						\begin{bmatrix}
							0 & -1 & 1 \\
							-3 & 2 - \lambda & -1\\
							-3 & 2 & -1-\lambda
						\end{bmatrix}
						+1\cdot\text{det}
						\begin{bmatrix}
							0 & -1 & 1\\
							1-\lambda & -1 & 1\\
							-3 & 2 & -1-\lambda
						\end{bmatrix} \\ \\
						&= \medmath{ (2-\lambda)\Bigg((1-\lambda)\big[ (2-\lambda)(-1-\lambda) +2 \big] 
													  + 3\big[ (1+\lambda) - 2 \big]
													  - 3\big[ 1 - (2-\lambda) \big] \Bigg)}
						\medmath{ -1\Bigg( 0 + 3 \big[ (1+\lambda) -2 \big]
											  - 3 \big[ 1 - (2-\lambda) \big] \Bigg) } \\
						&\medmath{\ \ + 1\Bigg( 0 - (1-\lambda)\big[ (1+\lambda) - 2 \big] - 0 \Bigg)} = 
						\medmath{(2-\lambda)(1-\lambda)(\lambda^2-\lambda) - 0  + (\lambda - 1)^2 }	=
						\medmath{\lambda^4 - 4\lambda^3 + 6\lambda^2 - 4\lambda + 1}\\ \\
						&=(\lambda - 1)^4 
					\end{align*}
					Ho quindi che: $\ P_A=(\lambda - 1)^4 \quad \lambda_{1,2,3,4} = 1 \quad \mu_a = 4 $.
					\item Procedo con il calcolo della molteplicità geometrica $ \mu_g $ dell'autovalore $ \lambda=1 $ calcolando lo spazio nullo generato:
					\[
						N(\vec{A} - 1\vec{I}) =
						N \begin{pmatrix}
							1 & 0 & -1 & 1 \\
							1 & 0 & -1 & 1 \\
							1 & -3 & 1 & -1 \\
							0 & -3 & 2 & -2
						\end{pmatrix}
					\]
					Devo quindi trovare una base che mi rappresenta lo spazio nullo. Utilizzo il metodo di \textit{Echelon} per portare il sistema in forma triangolare.
					
					\begin{align*}
						&\begin{pmatrix}
							1 & 0 & -1 & 1 \\
							1 & 0 & -1 & 1 \\
							1 & -3 & 1 & -1 \\
							0 & -3 & 2 & -2
						\end{pmatrix}
						\xrightarrow{E_{2 1}(-1)}
						\begin{pmatrix}
							1 & 0 & -1 & 1 \\
							0 & 0 & 0 & 0 \\
							1 & -3 & 1 & -1 \\
							0 & -3 & 2 & -2
						\end{pmatrix}
						\xrightarrow{E_{3 1}(-1)}
						\begin{pmatrix}
							1 & 0 & -1 & 1 \\
							0 & 0 & 0 & 0 \\
							0 & -3 & 2 & -2 \\
							0 & -3 & 2 & -2
						\end{pmatrix} \\
						&\xrightarrow{E_{4 3}(-1)}
						\begin{pmatrix}
							1 & 0 & -1 & 1 \\
							0 & 0 & 0 & 0 \\
							0 & -3 & 2 & -2 \\
							0 & 0 & 0 & 0
						\end{pmatrix}
						\xrightarrow{E_{3 2}}
						\begin{pmatrix}
							1 & 0 & -1 & 1 \\
							0 & -3 & 2 & -2 \\
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0
						\end{pmatrix}
						\xrightarrow{E_{2}(-1/3)}
						\begin{pmatrix}
							1 & 0 & -1 & 1 \\
							0 & 1 & -2/3 & 2/3 \\
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0
						\end{pmatrix}
					\end{align*}
					Ho quindi il seguente sistema di equazioni:
					\[
						\begin{cases*}
							x_1 - x_3 + x_4 = 0 \\
							x_2 - \dfrac{2}{3}x_3 + \dfrac{2}{3}x_4 = 0 \\
							x_3 = \alpha \\
							x_4 = \beta
						\end{cases*}
						\qquad\longrightarrow\qquad
						\begin{cases*}
							x_1 = x_3 - x_4 \\
							x_2 = \dfrac{2}{3}x_3 - \dfrac{2}{3}x_4 \\
							x_3 = \alpha \\
							x_4 = \beta
						\end{cases*}
						\qquad\longrightarrow\qquad
						\begin{matrix}
							x_1 \\
							x_2 \\
							x_3 \\
							x_4
						\end{matrix}
						\begin{bmatrix}
							1 \\
							2/3 \\
							1 \\
							0
						\end{bmatrix}
						\quad
						\begin{bmatrix}
							-1 \\
							-2/3 \\
							0 \\
							1
						\end{bmatrix}
					\]
					i vettori che rappresentano la base dello spazio nullo \textit{normalizzati} risultano essere quindi
					\[
						\left\{
						\begin{bmatrix}
							3 \\
							2 \\
							3 \\
							0
						\end{bmatrix},
						\begin{bmatrix}
							-3 \\
							-2 \\
							0 \\
							3
						\end{bmatrix}
						\right\}
					\]
					Ricavo quindi che $ \mu_g = 2 $ per $ \lambda_1 $. Perciò so che avrò due catene di Jordan e che la matrice di Jordan avrà due mini-blocchi.
					\item Siccome $\ \text{dim}(N) (= 2) < \mu_a (= 4)\ $ allora calcolo $ N(\vec{A} - 1\vec{I})^2 $:
					\begin{align*}
						N(\vec{A} - 1\vec{I})^2 &= N
						\begin{pmatrix}
							\begin{bmatrix}
								1 & 0 & -1 & 1 \\
								1 & 0 & -1 & 1 \\
								1 & -3 & 1 & -1 \\
								0 & -3 & 2 & -2
							\end{bmatrix}^2
						\end{pmatrix}
						= N
						\begin{pmatrix}
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0 \\
							-1 & 0 & 1 & -1 \\
							-1 & 0 & 1 & -1
						\end{pmatrix}
						\quad\xrightarrow{E_{3 1}}
						\begin{pmatrix}
							-1 & 0 & 1 & -1 \\
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0 \\
							-1 & 0 & 1 & -1
						\end{pmatrix} \\
						&\quad\xrightarrow{E_{4 2}}
						\begin{pmatrix}
							-1 & 0 & 1 & -1 \\
							-1 & 0 & 1 & -1\\
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0 
						\end{pmatrix} 
						\quad\xrightarrow{E_{2 1}(-1)}
						\begin{pmatrix}
							-1 & 0 & 1 & -1 \\
							0 & 0 & 0 & 0\\
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0 
						\end{pmatrix}
					\end{align*}
					Ho quindi il sistema:
					\[
						\begin{cases}
							x_1 = -x_3 + x_4 \\
							x_2 = \alpha \\
							x_3 = \beta \\
							x_4 = \gamma
						\end{cases}
						\quad
						\text{perciò la base di } N \text{ è }
						\left\{
						\begin{bmatrix}
							0\\
							1\\
							0\\
							0
						\end{bmatrix}
						\begin{bmatrix}
							1\\
							0\\
							1\\
							0
						\end{bmatrix}
						\begin{bmatrix}
							-1\\
							0\\
							0\\
							1
						\end{bmatrix}
						\right\}
					\]
					Però $ \text{dim}(N) (=3) < \mu_a (=4) $ perciò calcolo $ N(\vec{A} - 1\vec{I})^3 $:
					\[
						N(\vec{A} - 1\vec{I})^3 =
						N\begin{pmatrix}
							\begin{bmatrix}
								0 & 0 & 0 & 0 \\
								0 & 0 & 0 & 0 \\
								-1 & 0 & 1 & -1 \\
								-1 & 0 & 1 & -1
							\end{bmatrix}
							\begin{bmatrix}
								1 & 0 & -1 & 1 \\
								1 & 0 & -1 & 1 \\
								1 & -3 & 1 & -1 \\
								0 & -3 & 2 & -2
							\end{bmatrix}
						\end{pmatrix}
						=
						N
						\begin{pmatrix}
							\begin{bmatrix}
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0 \\
							0 & 0 & 0 & 0
							\end{bmatrix}
						\end{pmatrix}
						= \varnothing
					\]
					Quindi abbiamo che la base per lo spazio nullo è:
					\[
						\left\{
						\begin{bmatrix}
							1 \\ 0 \\ 0 \\ 0
						\end{bmatrix},
						\begin{bmatrix}
							0 \\ 1 \\ 0 \\ 0
						\end{bmatrix},
						\begin{bmatrix}
							0 \\ 0 \\ 1 \\ 0
						\end{bmatrix},
						\begin{bmatrix}
							0 \\ 0 \\ 0 \\ 1
						\end{bmatrix}
						\right\}
					\]
					posso quindi fermarmi dato che $\ \text{dim}(N) = \mu_a $.
					\item Scelgo un vettore $ v \in N(\vec{A} - 1\vec{I})^3 $ che non è presente (non è combinazione lineare di) nello spazio nullo $ N(\vec{A} - 1\vec{I})^2  $. \\Perciò scelgo $\quad  \begin{bmatrix}
					0 \\ 0 \\ 0 \\ 1
					\end{bmatrix} $.
					
					\item Calcolo a ritroso la catena di Jordan ricavando gli altri vettori sapendo che $ \vec{v}_{n-1} = (\vec{A} - 1 )\vec{v}_n $
					\[
						\vec{v}_3 =
						\begin{bmatrix}
							0 \\ 0 \\ 0 \\ 1
						\end{bmatrix}
						\qquad
						\vec{v}_2 =
						\begin{bmatrix}
							1 & 0 & -1 & 1 \\
							1 & 0 & -1 & 1 \\
							1 & -3 & 1 & -1 \\
							0 & -3 & 2 & -2
						\end{bmatrix}
						\begin{bmatrix}
							0 \\ 0 \\ 0 \\ 1
						\end{bmatrix}
						=
						\begin{bmatrix}
							1 \\ 1 \\ -1 \\ 2
						\end{bmatrix}
						\qquad
						\vec{v}_1 =
						\begin{bmatrix}
						1 & 0 & -1 & 1 \\
						1 & 0 & -1 & 1 \\
						1 & -3 & 1 & -1 \\
						0 & -3 & 2 & -2
						\end{bmatrix}
						\begin{bmatrix}
							1 \\ 1 \\ -1 \\ 2
						\end{bmatrix}
						=
						\begin{bmatrix}
							4 \\ 4 \\ -5 \\ -9
						\end{bmatrix}
					\]
					
					aggiungo poi un altro vettore linearmente indipendente dagli altri:$\quad \begin{bmatrix}
					1 \\ 0 \\ 0 \\ 0
					\end{bmatrix}  = \vec{v}_0$
					
					\item Ottengo così la matrice di trasformazione $ \vec{T} $:
					\[
					    \vec{T}=
						\begin{bmatrix}
							1 & 4 & 1 & 0 \\
							0 & 4 & 1 & 0 \\
							0 & -5 & -1 & 0 \\
							0 & -9 & 2 & 1
						\end{bmatrix}
						=
						\begin{bmatrix}
							\vec{v}_0 & \vec{v}_1 & \vec{v}_2 & \vec{v}_3 
						\end{bmatrix}
					\]
				\end{enumerate}
				
				
			\end{exmp}
			
		\newpage
		
		\section{Stabilità}
			L’importanza di tale proprietà deriva dal fatto che la stabilità
			è una specifica imposta a quasi ogni sistema fisico controllato perché implica la possibilità di lavorare intorno a certe condizioni nominali senza discostarsi troppo da esse.\\
			Nel seguito verranno introdotte due diverse definizioni di stabilità: la prima relativa al legame ingresso-uscita (stabilità BIBO), la seconda relativa ad una rappresentazione in termini di variabili di stato (stabilità alla Lyapunov).
			
			\subsection{Stabilità BIBO}
				\begin{defn}[stabilità BIBO]
					Un sistema SISO è detto BIBO (bounded-input bounded-output)
					stabile se e solo se a partire da una condizione di riposo, ad ogni ingresso limitato
					risponde con un’uscita anch’essa limitata.
				\end{defn}
				
				\begin{thm}[condizione necessaria e sufficiente per la BIBO stabilità]
					Si consideri un sistema SISO lineare e stazionario. La \textit{condizione necessaria e sufficiente} affinché tale sistema sia BIBO stabile è che la sua risposta impulsiva sia \textbf{assolutamente sommabile}, ovvero:
					\[
						\exists M>0\ \text{ t.c.}\ \int^{\infty}_0 \lvert \omega (\tau) \rvert d\tau\ \leq\ M\ <\ \infty
					\]
				\end{thm}
				
				\begin{prop}[condizione necessaria e sufficiente per l'assoluta sommabilità di $ \omega(t) $]
					La condizione necessaria e sufficiente per l'assoluta sommabilità di $ \omega(t) $ è che $ \omega(t) $ contenga tutti i modi del sistema, ovvero $ A_{i, k} \neq 0\quad \forall i\in 1,\dots, r\quad \forall K= 0,1,\dots,\nu_i-1 $
				\end{prop}
				
				\begin{thm}
					La \textit{condizione necessaria e sufficiente} affinché un sistema SISO, lineare, stazionario a parametri concentrati e in forma minima sia BIBO stabile:
					\begin{itemize}
						\item Tutte le \textbf{radici} del \textit{polinomio caratteristico} siano a \textit{parte reale \textbf{negativa}}.
						\item Tutti i \textbf{poli} della \textit{funzione caratteristica} siano a \textit{parte reale \textbf{strettamente negativa}}.
					\end{itemize}
				\end{thm}
				
			\subsection{Lyapunov in termini di variabili di stato}
				\begin{defn}[Sistema Autonomo]
					Un sistema nella forma:
				\[
					\vec{\dot{x}} (t) = f(\vec{x}(t), \vec{u}(t), t)
				\]
				è \textbf{autonomo} se:
				\begin{itemize}
					\item $ \vec{u}(t) $ è identicamente \textit{nullo}.
					\item è \textit{stazionario}, ovvero $ f $ \textit{non dipende esplicitamente dal tempo}.
				\end{itemize}
				Quindi il sistema diventa:
				\[
					\vec{\dot{x}}(t) = f(\vec{x}(t))
				\]
				\end{defn}
				
				\begin{defn}[Stato di equilibrio]
					Uno stato $ \vec{x}_e $ è uno \textbf{stato di equilibrio} (o punto di equilibrio) per il sistema se vale:
					\[
						\vec{x}(\tau) = \vec{x}_e\ \Rightarrow\ (\forall t\geq \tau)\quad \vec{x}(t) = \vec{x}_e
					\]
					Ovvero se ogni traiettoria che parte da $ \vec{x}_e $ in tempo $ \tau $ resta in $ \vec{x}_e $ all'istante successivo.
				\end{defn}
				
				\begin{itemize}
					\item Uno stato di equilibrio è \textbf{stabile} se 
					\[
					\forall\epsilon > 0 \quad\exists \delta(\epsilon) > 0\ \text{ t.c. se }\ \| \vec{x}(\vec{0}) - \vec{x}_e \| \leq \delta(\epsilon)\ \text{ allora }\ \| \vec{x}(t) - \vec{x}_e \| < \epsilon\quad  \forall t\geq 0
					\]
					\item L'\textit{origine} di uno stato di equilibrio è \textbf{stabile} se 
					\[
					\forall\epsilon > 0 \quad\exists \delta(\epsilon) > 0\ \text{ t.c. se }\ \| \vec{x}(\vec{0}) \| \leq \delta(\epsilon)\ \text{ allora }\ \| \vec{x}(t) \| < \epsilon\quad  \forall t\geq 0
					\]
					\item Uno stato di equilibrio è \textbf{asintoticamente stabile} se \textit{è stabile} e
					$\ \lim\limits_{t\rightarrow\infty} \| \vec{x}(t) - \vec{x}_e \| = 0 $
					
					\item L'origine di uno stato di equilibrio è \textbf{asintoticamente stabile} se \textit{è stabile} e
					$\ \lim\limits_{t\rightarrow\infty} \| \vec{x}(t) \| = 0 $
					
				\end{itemize}
				
				\begin{prop}
					Se uno stato di equilibrio $ \vec{x}_e $ è \textit{asintoticamente stabile}, qualunque sia lo stato iniziale da cui la traiettoria ha origine, allora $ \vec{x}_e $ è \textbf{globalmente asintoticamente stabile}. Ne deriva che $ \vec{x}_e $ è \textit{l'unico} stato di equilibrio del sistema.
				\end{prop}
				
			
			\subsection{Lyapunov per sistemi lineari e stazionari}	
			
				\begin{prop}
					Se $ \ \dot{\vec{x}} = \vec{A}\vec{x}(t)\ $ è un sistema \textit{lineare autonomo}, $ \vec{x}_e $ è un punto di equilibrio sse vale
					\[
						\vec{A}\vec{x}_e  = \vec{0}
					\]
					dove $ \vec{x}_e $ è la soluzione di tale sistema lineare omogeneo.
				\end{prop}
				
				\noindent
				Ricaviamo dunque che:
				\begin{itemize}
					\item Se $ \vec{A} $ \textit{non} è singolare (determinante $ \neq 0 $), l'unico punto di equilibrio è $\ \vec{x}_e = \vec{0}\ $ (l'origine).
					\item Se $ \vec{A} $ è singolare, il sistema ha un numero infinito di stati di equilibrio che descrivono uno \textbf{spazio lineare}: tutti questi stati sono contenuti nello spazio nullo $\ N(\vec{A}) $.
				\end{itemize}
	
				\begin{prop}
					Dato un sistema \textit{lineare autonomo}:
					\begin{itemize}
						\item Se uno stato di equilibrio è \textit{stabile} (instabile) $\ \Rightarrow\ $ tutti gli altri $ \vec{x}_e $ sono \textit{stabili} (instabili).
						\item Se uno stato di equilibrio è \textit{asintoticamente stabile} allora valgono:
						\begin{itemize}
							\item $ \vec{x}_e = \vec{0} $.
							\item $ \vec{x}_e $ è l'unico stato di equilibrio.
							\item $ \vec{x}_e $ è globalmente asintoticamente stabile.
						\end{itemize}
					\end{itemize}
				\end{prop}
				
				\begin{prop}
					Dato il sistema lineare \textit{stazionario}: $ \ \dot{\vec{x}}(t) = \vec{A}\vec{x}(t) $:
					\begin{itemize}
						\item È \textbf{asintoticamente stabile} sse tutti gli \textit{autovalori} di $ \vec{A} $ hanno \textit{parte reale negativa}.
						\item È \textbf{stabile} sse $ \vec{A} $ \textit{non} ha autovalori a parte \textit{reale positiva} e gli eventuali autovalori nulli hanno \textbf{indice unitario} (molteplicità $ \mu_g = 1 $).
						\item È \textbf{instabile} sse $ \vec{A} $ ha \textit{almeno un} autovalore a parte reale positiva \textit{oppure} parte reale nulla \textit{e} indice non unitario.
					\end{itemize}
				\end{prop}
				
			\subsection{Stabilità con la funzione di Lyapunov}
				\begin{defn}[Funzione definita positiva]
					La funzione $\ V(\vec{x})\ $ scalare, continua è \textbf{definita positiva} in $ \vec{x}' $ se $ \exists $ una regione $\ \Omega\ $ dello spazio di stato (intorno circolare di $ \vec{x}' $) tale che
					\[
						V(\vec{x}) > 0 \quad \forall \text{ stato } \vec{x}\neq \vec{x}' \text{ in } \Omega\ \text{ e } V(\vec{x}') = 0
					\]
					Se $ \Omega $ coincide con l'intero spazio di stato allora $ V(\vec{x}) $ è \textbf{globalmente definita positiva}.
				\end{defn}
				
				\begin{defn}[Funzione semi-definita positiva]
					La funzione $\ V(\vec{x}) \ $ scalare, continua è \textbf{semi-definita positiva} in $ \vec{x}' $ se $ \exists $ una regione $\ \Omega\ $ dello spazio di stato (intorno circolare di $ \vec{x}' $) tale che
					\[
						V(\vec{x}) \geq 0 \quad \forall \text{ stato } \vec{x}\neq \vec{x}' \text{ in } \Omega\ \text{ e } V(\vec{x}') = 0 
					\]
					Se $ \Omega $ coincide con l'intero spazio di stato allora $ V(\vec{x}) $ è \textbf{globalmente semi-definita positiva}.
				\end{defn}
				
				\begin{defn}[Funzione (globalmente) (semi-)definita negativa]
						La funzione $\ V(\vec{x}) \ $ scalare, continua è \textbf{(globalmente) (semi)-definita negativa} in $ \vec{x}' $ se $\ -V(\vec{x})\ $ è (globalmente) (semi-)definita positiva in $ \vec{x}' $.
				\end{defn}
				
				\begin{thm}[Metodo di Lyapunov]
					Considero il sistema autonomo $\ \dot{\vec{x}}(t) = \vec{f}(\vec{x}(t)) $ dove $ \vec{f}(\cdot) $ è \textit{continuo}, con le derivate parziali prime $ \dfrac{\partial \vec{f}}{\partial x_i}\ \ i=1,\dots,n $.\\
					Sia $ \vec{x}_e $ un punto di equilibrio per il sistema, cioè $ \ \vec{f}(\vec{x}_e) = \vec{0}\ \ \forall t\geq 0 $.\\
					Se esiste una \textit{funzione scalare continua} $ \ V(\vec{x}) $ continua insieme alle sue derivate parziali prime, \textit{definita positiva} in $ \vec{x}_e $ e tale che
					\[
						\dot{V}(\vec{x}) = \dv{V(\vec{x})}{t} = \partialderivative{V(\vec{x})}{\vec{x}}\dv{\vec{x}}{t} =
						\partialderivative{V(\vec{x})}{\vec{x}}\cdot \vec{f}(\vec{x}) = 
						\partialderivative{V}{x_1}\dot{x}_1 + \cdots + \partialderivative{V}{x_n}\dot{x}_n
					\]
					\begin{itemize}
						\item Se $\ \dot{V}(\vec{x})\ $ è \textbf{semi-definita negativa} in $ \vec{x}_e $
						$ \ \Rightarrow\ $ $ \vec{x}_e $ è uno \textit{stato di equilibrio} \textbf{stabile}.
						\item Se $\ \dot{V}(\vec{x})\ $ è \textbf{definita negativa} in $ \vec{x}_e $
						$ \ \Rightarrow\ $ $ \vec{x}_e $ è uno \textit{stato di equilibrio} \textbf{asintoticamente stabile}.
						\item Se $\ \dot{V}(\vec{x})\ $ è \textbf{definita positiva} in $ \vec{x}_e $
						$ \ \Rightarrow\ $ $ \vec{x}_e $ è uno \textit{stato di equilibrio} \textbf{instabile}.
					\end{itemize}
					
				\end{thm}
				
			\subsection{Matrice di linearizzazione o Jacobiana}
				La matrice di linearizzazione o matrice Jacobiana o Jacobiano di $ \vec{f}(\cdot) $ calcolata in $ \vec{x}_e $ è definita nel seguente modo:
				\[
					\vec{J}(\vec{x}_e) = \bigg[ \partialderivative{\vec{f}}{\vec{x}} \bigg]_{\vec{x}_e} =
					\left[
					\begin{BMAT}(,27pt, 27pt){cccc}{cccc}
						\partialderivative{f_1}{x_1} & \partialderivative{f_1}{x_2} & \cdots & \partialderivative{f_1}{x_n} \\
						\partialderivative{f_2}{x_1} & \partialderivative{f_2}{x_2} & \cdots & \partialderivative{f_2}{x_n} \\
						\vdots & \vdots & \ddots & \vdots \\
						\partialderivative{f_n}{x_1} & \partialderivative{f_n}{x_2} & \cdots & \partialderivative{f_2}{x_n}
					\end{BMAT}
					\right]_{\vec{x}_e}
				\]
				\begin{itemize}
					\item Se $ \vec{J} $ ha autovalori \textit{tutti} a parte reale \textit{negativa} $ \ \Rightarrow\ $ $ \vec{x}_e $ è \textbf{asintoticamente stabile}.
					\item Se $ \vec{J} $ ha \textit{almeno un} autovalore a parte reale \textit{positiva} $ \ \Rightarrow\ $ $ \vec{x}_e $ è \textbf{instabile}.
				\end{itemize}
			
			\newpage
				
		\section{Controllabilità}
			\begin{defn}[Sistema controllabile]
				Un sistema \textit{lineare stazionario} 
				\[
					\dot{\vec{x}}(t) = \vec{A}\vec{x}(t) + \vec{B}\vec{u}(t)
				\] 
				è \textbf{controllabile} sse è possibile, agendo sull'ingresso, trasferire lo stato del sistema da un qualunque stato iniziale $ \ \vec{x}_0 = \vec{x}(0)\ $ ad un qualunque altro stato $\ \vec{x}_f = \vec{x}(t_f)\ $, detto \textit{stato zero} o \textit{stato obiettivo}, in un tempo finito $ t_f\geq 0 $. 
			\end{defn}
			
			\begin{defn}[Gramiano di controllabilità]
				Dato un sistema $\ \dot{\vec{x}}(t) = \vec{A}\vec{x}(t) + \vec{B}\vec{u}(t)\ $ dove $\ \vec{x}\in\R^n\ $ e $\ \vec{u}\in\R^r\ $, il \textbf{gramiano di controllabilità} è la matrice $ n\times n $:
				\[
					\vec{\Gamma}(t) = \int_{0}^{t} e^{\vec{A}\tau} \vec{B}\vec{B}^T e^{\vec{A}^T\tau}d\tau
				\]
				Il sistema è \textbf{controllabile} se il \textit{gramiano non è singolare} $ \forall t > 0 $.
			\end{defn}
				
			\begin{defn}[Matrice di controllabilità]
				La matrice di controllabilità è una $ (n)\times(r\cdot n) $ definita come segue:
				\[
					\mathcal{T} = 
					\begin{bmatrix}
						\vec{B} & \vec{A}\vec{B} & \vec{A}^2\vec{B} & \cdots & \vec{A}^{n-1}\vec{B}
					\end{bmatrix}
				\]
				Condizione \textit{necessaria e sufficiente} affinché il sistema sia controllabile:
				\[
					n_c \stackrel{\text{def}}{=} \text{rango}(\mathcal{T}) = n
				\]
			\end{defn}
			
			\begin{prop}
				Dato il sistema:
				\[
					\dot{\vec{x}}(t) =
					\begin{bmatrix}
						\lambda_1 & 0 & \cdots & 0 \\
						0 & \lambda_2 & \cdots & 0 \\
						\vdots & \vdots & \ddots & \vdots \\
						0\tikzmark{lowerA} & 0 & \cdots & \lambda_n\tikzmark{lowerB}
					\end{bmatrix}
					\vec{x}(t) +
					\begin{bmatrix}
						b_{11} & b_{12} & \cdots & b_{1r} \\
						b_{21} & b_{22} & \cdots & b_{2r} \\
						\vdots & \vdots & \ddots & \vdots \\
						b_{n1}\tikzmark{lowerC} & b_{n2} & \cdots & b_{nr}\tikzmark{lowerD} \\
					\end{bmatrix}
					\vec{u}(t)
				\]
				\begin{tikzpicture}[overlay, remember picture,decoration={brace,amplitude=5pt}]
				\draw[decorate,thick] ($ (lowerB.south) +(.20,-.25) $) -- ($ (lowerA.south) +(-.250,-.25) $)
				node [midway,below=5pt] {A};
				\draw[decorate,thick] ($ (lowerD.south) +(.250,-.25) $) -- ($ (lowerC.south) +(-.50,-.25) $)
				node [midway,below=5pt] {B};
				\end{tikzpicture}
				
				\vspace{1cm}
				dove $ \vec{A} $ è in forma diagonale con autovalori tutti distinti, la condizione necessaria e sufficiente affinché sia \textbf{controllabile} è che $ \vec{B} $ \textit{non} abbia \textit{righe identicamente nulle}.
			\end{prop}
			
			\begin{prop}
				La trasformazione di \textbf{similitudine} preserva la controllabilità.
			\end{prop}
			
			\begin{defn}[Forma canonica di Kalman]
				Dato il sistema
				\[
					\dot{\vec{z}}(t) = \vec{A}'\vec{z}(t) + \vec{B}'\vec{u}(t)
				\]
				è nella forma canonica di Kalman se:
				\[
					\vec{A}' =
					\left[
					\begin{BMAT}(e,10pt,10pt){c1c}{c1c}
					\vec{A}_c' & \vec{A}_1' \\
					\vec{0} & \vec{A}_{nc}'
					\end{BMAT}
					\right]
					\qquad
					\vec{B}' =
					\left[
					\begin{BMAT}(e,10pt,10pt){c}{c1c}
						\vec{B}_c'\\
						0
					\end{BMAT}
					\right]
				\]
			\end{defn}
			dove:
			\begin{itemize}
				\item $ \vec{A}_c' $ è quadrata con ordine uguale al rango $ n_c $ della matrice di controllabilità $ \mathcal{T} $.
				\item $ \vec{B}_c' $ ha numero di \textit{righe} uguale al rango $ n_c $ della matrice di controllabilità $ \mathcal{T} $.
			\end{itemize}
			In particolare la coppia $ (\vec{A}_c', \vec{B}_c') $ è controllabile.
			
			Il vettore $ \vec{z} $ può essere riscritto come:
			\[
				\vec{z} =
				\left[
				\begin{BMAT}(e,10pt,10pt){c}{c1c}
					\vec{z}_c\tikzmark{A}\\
					\vec{z}_{nc}\tikzmark{B}
				\end{BMAT}
				\right]
			\]
			\begin{tikzpicture}[overlay, remember picture, >=latex]
			\draw[thick, ->] (A.east) -- +(1,0) node [right] {$ \in \R^{n_c} $};
			\draw[thick, ->] (B.east) -- +(1,0)	node [right] {$ \in \R^{n - n_c} $};
			\end{tikzpicture}
				
			Quindi il sistema può essere decomposto in due parti:
			\[
			\begin{cases}
				\dot{\vec{z}}_c (t) = \vec{A}_c'\vec{z}_c(t) + \vec{A}_1'\vec{z}_{nc}(t) + \vec{B}_c'\vec{u}(t) \qquad &
				\text{ Parte controllabile di ordine } n_c \\  
				\dot{\vec{z}}_{nc}(t) = \vec{A}_{nc}'\vec{z}_{nc}(t) \qquad &
				\text{ Parte non controllabile di ordine } n - n_c
			\end{cases}
			\]
			\bigskip
			
			\tikzset{
				block/.style = {draw, fill=white, rectangle, minimum height=3em, minimum width=3em},
				tmp/.style  = {coordinate}, 
				sum/.style= {draw, fill=white, circle, node distance=2cm},
				input/.style = {coordinate},
				output/.style= {coordinate},
				pinstyle/.style = {pin edge={to-,thin,black}},
				triangle/.style = {draw, regular polygon, regular polygon sides=3, node distance=2cm,shape border rotate=-90, minimum height=3em, minimum width=3em }
			}
			
			\begin{tikzpicture}[auto, node distance=2cm,>=latex',decoration={brace,amplitude=10pt}]
				\node[input, name=rinput] (rinput) {};
				\node[block, right of=rinput] (B) {$ \vec{B}_c' $};
				\node[sum, right of=B] (sum1) {};
				\node[triangle, right of=sum1] (int1) {$ \int $};
				\node[block, below of=int1] (Ac) {$ \vec{A}_c' $};
				\node[block, below of=Ac] (A1) {$ \vec{A}_1' $};
				\node[triangle, below of=A1] (int2) {$ \int $}; 
				\node[block, below of=int2] (Anc) {$ \vec{A}_{nc}' $};
				\node[input, name=routput, right of=int1, xshift=1cm] (routput) {};
				\node[input, name=routput1, right of=A1, xshift=1cm] (routput1) {};
				\draw[->] (rinput) -- node[above] {$ \vec{u} $} (B);	\draw[->] (B) -- node[above, xshift=.5cm] {$ + $} (sum1) ; 
				\draw[->] (sum1) -- node[above] {$ \dot{\vec{z}}_{c} $} (int1); 	\draw[->] (int1) --  (routput) node[above] {$ \vec{z}_c $};
				\draw[->] ($ (int1) +(2,0) $) -- ($ (Ac) +(2,0) $) -- (Ac.east);
				\draw[->] (Ac.west) -- ($ (sum1) +(0,-2) $) -- node[right, yshift=.65cm] {$ + $} (sum1.south);
				\draw (A1.west) -- ($ (sum1) +(0,-4) $) -- +(0,2);
				\draw[->] (A1) -- (routput1) node[above] {$ \vec{z}_{nc} $};
				\draw[->] ($ (A1) +(2,0) $) -- ($ (Anc) +(2,0) $) -- (Anc);
				\draw (int2) -- +(2,0); \draw[->] (Anc) -- +(-2,0) -- ($ (int2) +(-2,0) $) -- node[above] {$ \dot{\vec{z}} $} (int2);
				\draw[dashed] (0.5,1) -- ($ (rinput) + (0.5,-5) $) -- ($ (A1) +(1,-1) $) -- +(0,2) -- +(2.5,2) -- (9.5,1) -- cycle;
				\draw[dashed] ($ (B) +(1,-5) $) -- +(0,-4) -- ($ (routput1) +(0.5,-5) $) -- +(0,6);
				\draw[decorate,thick] (10,1) -- (10,-3)	node [midway,right=15pt] {Controllabile};
				\draw[decorate,thick] (10,-3) -- (10,-9) node [midway,right=15pt] {Non controllabile};
			\end{tikzpicture}
			
			\bigskip
			
			\noindent
			Dato un sistema $ \ \vec{\dot{x}}(t) = \vec{A}\vec{x}(t) + \vec{B}\vec{u}(t)\ $ esso può essere ricondotto alla forma canonica controllabile di Kalman attraverso la trasformazione di similitudine:
			\[
				\vec{x}(t) = \vec{P}\vec{z}(t)
			\]
			dove $\ \vec{P}\ $ è una matrice \textit{non singolare} le cui prime $ n_c $ colonne corrispondono alle prime $ n_c $ colonne linearmente indipendenti della matrice $ \mathcal{T} $, e le rimanenti $ n - n_c $ colonne sono linearmente indipendenti tra loro e dalle $ n_c $ colonne precedenti. La trasformazione che porta alla forma di Kalman \textit{non è unica}. 
			
		\subsection{Retroazione dello stato}
			\begin{tikzpicture}[auto, node distance=2cm,>=latex', scale=0.7, every node/.style={scale=0.8}]
				\node[input, name=rinput] (rinput) {};
				\node[sum, right of=rinput] (sum1) {};
				\node[block, right of=sum1] (B) {$ \vec{B} $};
				\node[sum, right of=B] (sum2) {};
				\node[triangle, right of=sum2] (int) {$ \int $};
				\node[block, right of=int, xshift=1.5cm] (C) {$ \vec{C} $};
				\node[block, below of=int] (A) {$ \vec{A} $};
				\node[block, below of=B, yshift=-1cm] (K) {$ \vec{K}(t) $};
				\node[output, right of=C] (routput) {};
				\draw[->] (rinput) node[above] {$ \vec{r} $} -- node[above, xshift=.7cm] {$ + $} (sum1) ;
				\draw[->] (sum1) -- (B) node[midway, above] {$ \vec{u} $};
				\draw[->] (B) -- node[above, xshift=.5cm] {$ + $}  (sum2);
				\draw[->] (sum2) -- (int) node[midway, above] {$ \dot{\vec{x}} $};
				\draw[->] (int) -- (C) node[midway, above] {$ \vec{x} $};
				\draw[->] (C) -- (routput) node[above] {$ \vec{y} $};
				\draw[->] ($ (int) +(1.75,0) $) -- ($ (A) +(1.75,0) $) -- (A);
				\draw[->] (A) -- ($ (sum2) +(0,-2.25) $) -- (sum2) node[right, yshift=-.25cm] {$ + $};
				\draw[->] ($ (B) +(6.5,0) $) -- ($ (K) +(6.5,0) $) -- (K);
				\draw[->] (K) -- +(-2.25,0) -- (sum1.south) node[right, yshift=-.25cm] {$ - $};
			\end{tikzpicture}
			
			Se $ \vec{r} = \vec{0} $ (set point) ho che $\ \vec{u}(t) = -\vec{K}(t)\vec{x}(t) $. L'equazione del sistema con retroazione è la seguente:
			\[
				\dot{\vec{x}}(t) = \vec{A}\vec{x}(t) + \vec{B}\vec{u}(t) = (\vec{A} - \vec{B}\vec{K}(t))\vec{u}(t)
			\]
			dove $ \vec{K}(t) $ è la \textbf{matrice di retroazione} $ \vec{K} \in \R^{r\times n} $, e $ \ (\vec{A} - \vec{B}\vec{K}(t))\ $ è detta \textit{matrice dinamica a ciclo chiuso}.
		
			Il sistema è controllabile sse scelto un qualunque insieme di $ n $ numeri reali e/o coppie di numeri complessi coniugati $\ \bar{\lambda}_1, \bar{\lambda}_2, \dots, \bar{\lambda}_n\ $ esiste una matrice di retroazione $ \vec{K} \in \R^{r\times n} $ tale che gli autovalori di $ (\vec{A} - \vec{B}\vec{K}(t)) $ siano pari a $\ \bar{\lambda}_1, \bar{\lambda}_2, \dots, \bar{\lambda}_n\ $.
			
		\subsection{Tutorial retroazione}
			\begin{enumerate}
				\item Vedo se il sistema è controllabile, cioè guardo il rango della matrice
				\[
					\mathcal{T} = 
					\begin{bmatrix}
						\vec{A} & \vec{A}\vec{B} & \vec{A}^2\vec{B} & \cdots & & \vec{A}^{n-1}\vec{B}
					\end{bmatrix}
				\]
				Se ha rango uguale all'ordine del sistema allora è controllabile.
				\item Calcolo il polinomio caratteristico e quello desiderato:
				\[
					P(s) = \text{det}(s\vec{I} - \vec{A}) \qquad P_c(s) = (s - \bar{\lambda}_1)(s - \bar{\lambda}_2)\cdots (s - \bar{\lambda}_n)
				\]
				da questi due mi ricavo: $ \ \alpha_0, \alpha_1, \dots, \alpha_{n-1}\ $ e $ \ \bar{\alpha}_0, \bar{\alpha}_1, \dots, \bar{\alpha}_{n-1} $.
				\item Trovo la matrice 
				\[
					\mathcal{T}_c^{-1} =
					\begin{bmatrix}
						\alpha_1 & \alpha_2 & \alpha_3 & \cdots & \alpha_{n-1} & 1 \\
						\alpha_2 & \alpha_3 & \vdots & \iddots & 1 & 0 \\
						\alpha_3 & \vdots & \alpha_{n-1} & \iddots & 0 & 0 \\
						\vdots & \alpha_{n-1} & 1 & \cdots & 0 & 0 \\
						\alpha_{n-1} & 1 & 0 & \cdots & 0 & 0 \\
						1 & 0 & 0 & \cdots & 0 & 0
					\end{bmatrix}
				\]
				\item Trovo
				\[
					\vec{K}_c = 
					\begin{bmatrix}
						\bar{\alpha}_0 - \alpha_0 & \bar{\alpha}_1 - \alpha_1 & \cdots & \bar{\alpha}_{n-1} - \alpha_{n-1}
					\end{bmatrix}
				\]
				\item Calcolo la matrice di retroazione
				\[
					\vec{K} =\vec{K}_c \vec{P}^{-1} = K_c (\mathcal{T}\mathcal{T}_c^{-1})
				\] 
				\item Calcolo le altre matrici:
				\begin{align*}
					& \vec{A}_c = \vec{P}^{-1} \vec{A} \vec{P} \qquad & \vec{C}_c = \vec{C}\vec{P} \\
					& \vec{B}_c = \vec{P}^{-1} \vec{B} \qquad & \vec{D}_c = \vec{D}
				\end{align*}
			\end{enumerate}
			
			\newpage
			
	\section{Osservabilità}
		\begin{defn}[Osservabilità]
			Un sistema lineare stazionario
			\[
				\begin{cases}
					\dot{\vec{x}}(t) = \vec{A}\vec{x}(t) \\
					\vec{y}(t) = \vec{C}\vec{x}(t)
				\end{cases}
			\]
			è detto \textbf{osservabile} sse, qualunque stato iniziale $ \ \vec{x}_0 = \vec{x}(\vec{0}) $, tale valore dello stato, può essere determinato sulla base dell'osservazione dell'evoluzione libera per un tempo finito $ \ t_f\geq 0 $.
		\end{defn}
		
		\begin{defn}[Gramiano di osservabilità]
			Dato un sistema lineare e stazionario $ \begin{cases}
				\dot{\vec{x}}(t) = \vec{A}\vec{x}(t) \\
				\vec{y}(t) = \vec{C}\vec{x}(t)
			\end{cases} $
			 dove $\ \vec{x} \in \R^n \ $ e $ \vec{y} \in \R^p $, il \textbf{gramiano di osservabilità} è la matrice $ n\times n $
			 \[
				 \vec{O}(t) = \int_0^t e^{\vec{A}^T\tau}\vec{C}^T\vec{C}e^{\vec{A}\tau}d\tau
			 \]
			 Il sistema è \textbf{osservabile} sse il \textit{gramiano non è singolare} $ \forall t > 0 $.
		\end{defn}
		
		\begin{defn}[Matrice di osservabilità]
			La matrice di osservabilità è una $\ (p\cdot n)\times n\  $ definita come segue:
			\[
				\mathcal{O} =
				\begin{bmatrix}
					\vec{C} \\\vec{C}\vec{A} \\ \vec{C}\vec{A}^2 \\ \vdots \\ \vec{C}\vec{A}^{n-1} \\
				\end{bmatrix}
			\]
			Condizione \textit{necessaria e sufficiente} affinché il sistema sia osservabile:
			\[
				n_o \stackrel{def}{=} \text{rango}(\mathcal{O}) = n
			\]
		\end{defn}
		
		\begin{prop}
			Dato il sistema con:
			\[
				\begin{bmatrix}
					\lambda_1 & 0 & \cdots & 0 \\
					0 & \lambda_2 & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & \lambda_n
				\end{bmatrix}\qquad \qquad
				\begin{bmatrix}
					c_{11} & c_{12} & \cdots & c_{1n} \\
					c_{21} & c_{22} & \cdots & c_{2n} \\
					\vdots & \vdots & \ddots & \vdots \\
					c_{n1} & c_{n2} & \cdots & c_{pn} \\
				\end{bmatrix}
			\]
			dove $ \vec{A} $ è in forma diagonale con autovalori tutti distinti, la condizione necessaria e sufficiente affinché sia \textbf{osservabile} è che $ \vec{C} $ \textit{non} abbia \textbf{colonne identicamente nulle}.
		\end{prop}
		
		\begin{prop}
			La trasformazione di \textbf{similitudine} preserva l'osservabilità.
		\end{prop}
		
		\begin{defn}[Forma canonica di Kalman]
			Dato il sistema
			\[
				\begin{cases}
					\dot{\vec{z}}(t) = \vec{A}'\vec{z}(t) \\
					\vec{y}(t) = \vec{C}'\vec{z}(t)
				\end{cases}
			\]
			è nella forma canonica di Kalman se:
			\[
				\vec{A}' =
				\left[
				\begin{BMAT}(e,10pt,10pt){c1c}{c1c}
					\vec{A}_o' & \vec{0} \\
					\vec{A}_1' & \vec{A}_{no}'
				\end{BMAT}
				\right]
				\qquad
				\vec{C}' =
				\left[
				\begin{BMAT}(e,10pt,10pt){c1c}{c}
					\vec{C}_o' & 0
				\end{BMAT}
				\right]
			\]
		\end{defn}
		dove:
		\begin{itemize}
			\item $ A_o' $ è quadrata con ordine uguale al rango $ n_o $ della matrice di osservabilità $ \mathcal{O} $.
			\item $ C_o ' $ ha numero di \textit{colonne} uguale al rango $ n_o $ della matrice di osservabilità $ \mathcal{O}$.
		\end{itemize}
		In particolare la coppia $ (\vec{A}_o', \vec{C}_o') $ è controllabile.
		
		Il vettore $ \vec{z} $ può essere riscritto come:
		\[
			\vec{z} =
			\left[
			\begin{BMAT}(e,10pt,10pt){c}{c1c}
				\vec{z}_o\tikzmark{A}\\
				\vec{z}_{no}\tikzmark{B}
			\end{BMAT}
			\right]
		\]
		\begin{tikzpicture}[overlay, remember picture, >=latex]
			\draw[thick, ->] (A.east) -- +(1,0) node [right] {$ \in \R^{n_o} $};
			\draw[thick, ->] (B.east) -- +(1,0)	node [right] {$ \in \R^{n - n_o} $};
		\end{tikzpicture}
		
		Quindi il sistema può essere decomposto in tre parti:
		\[
			\begin{cases}
				\dot{\vec{z}}_o (t) = \vec{A}_o'\vec{z}_o(t) \qquad &
				\text{ Parte osservabile di ordine } n_o \\  
				\dot{\vec{z}}_{no}(t) = \vec{A}_{no}'\vec{z}_{no}(t) + \vec{A}_1'\vec{z}_{o}(t) \qquad &
				\text{ Parte non osservabile di ordine } n - n_o \\
				\vec{y}(t) = \vec{C}_o'\vec{z}_o(t) \qquad &
				\text{ Trasformazione in uscita, non influenzata da } \vec{z}_{no}
			\end{cases}
		\]
		\bigskip
		
		\begin{tikzpicture}[auto, node distance=2cm,>=latex',decoration={brace,amplitude=10pt}]
			\node[input, name=rinput] (rinput) {};
			\node[block, right of=rinput] (A1) {$ \vec{A}_1' $};
			\node[sum, right of=A1] (sum1) {};
			\node[triangle, right of=sum1] (int1) {$ \int $};
			\node[block, below of=int1] (Ano) {$ \vec{A}_{no}' $};
			\node[triangle, below of=A1, yshift=-2cm] (int2) {$ \int $};
			\node[block, below of=Ano] (Co) {$ \vec{C}_{o}' $};
			\node[block, below of=int2] (Ao) {$ \vec{A}_{o}' $};
			\node[output, right of=Co, xshift=1cm] (routput) {};
			\draw[->] (rinput) -- (A1);\draw[->] (A1) -- node[above,xshift=.5cm] {$ + $} (sum1); 
			\draw[->] (sum1) -- (int1) node[above, midway] {$ \dot{\vec{z}}_{no} $};
			\draw[->]  (int1) node[above,xshift=1.5cm] {$ \vec{z}_{no} $} -- +(2,0) -- ($ (Ano) +(2,0) $) -- (Ano);
			\draw[->] (Ano) -- ($ (sum1) +(0,-2) $) -- (sum1) node[right, yshift=-.35cm] {$ + $};
			\draw (rinput) -- +(0,-2.5) -- +(4,-2.5) -- +(4,-4);
			\draw[->] (int2) -- (Co) node[above, midway,xshift=-.5cm] {$ \vec{z}_o $}; \draw[->] (Co) -- (routput) node[above] {$ \vec{y} $};
			\draw[->] ($ (int2) +(1.5,0)$) -- +(0,-2) -- (Ao);
			\draw[->] (Ao) -- +(-2,0) -- +(-2,2) -- (int2) node[above, midway] {$ \dot{\vec{z}}_{o} $} ;
			\draw[dashed] (-1,1) rectangle (9.5,-3);
			\draw[dashed] (-1,-3) -- (-1,-7) -- (9.5,-7) -- (9.5,-3);
			\draw[decorate,thick] (10,1) -- (10,-3)	node [midway,right=15pt] {Non Osservabile};
			\draw[decorate,thick] (10,-3) -- (10,-7) node [midway,right=15pt] {Osservabile};
		\end{tikzpicture}
			\bigskip
			
			\noindent
			Dato un sistema $ \begin{cases}
			\dot{\vec{x}}(t) = \vec{A}\vec{x}(t) \\
			\vec{y}(t) = \vec{C}\vec{x}(t)
			\end{cases} $ esso può essere ricondotto alla forma canonica osservabile di Kalman attraverso la trasformazione di similitudine:
			\[
			\vec{x}(t) = \vec{P}\vec{z}(t)
			\]
			dove $\ \vec{P}\ $ è una matrice \textit{non singolare} le cui prime $ n_o $ colonne corrispondono alle prime $ n_o $ righe trasposte linearmente indipendenti della matrice $ \mathcal{O} $, e le rimanenti $ n - n_o $ colonne sono linearmente indipendenti tra loro e dalle $ n_o $ colonne precedenti. La trasformazione che porta alla forma di Kalman \textit{non è unica}. 
			
	\subsection{Dualità controllabilità-osservabilità}
		Dati due sistemi:
		\[
			S_1
			\begin{cases}
				\dot{\vec{x}}(t) = \vec{A}\vec{x}(t) + \vec{B}\vec{u}(t) \\
				\vec{y}(t) = \vec{C}\vec{x}(t)
			\end{cases}
			\qquad\qquad
			S_2
			\begin{cases}
				\dot{\vec{z}}(t) = \vec{A}^T\vec{z}(t) + \vec{C}^T\vec{v}(t) \\
				\vec{s}(t) = \vec{B}^T\vec{z}(t)
			\end{cases}
		\]
		con $ \ \vec{x}\in\R^n, \vec{u}\in\R^r, \vec{y}\in\R^p, \vec{A}\in\R^{n\times n}, \vec{B}\in\R^{n\times r}, \vec{C}\in\R^{p\times n}, \vec{z}\in\R^n, \vec{v}\in\R^r, \vec{s}\in\R^p $.\\
		$ S_1 $ è \textbf{controllabile} (osservabile) sse $ S_2 $ è \textbf{osservabile} (controllabile).
		
	\subsection{Osservatore asintotico dello stato (Luenberger)}
		\begin{defn}[Osservatore asintotico]
		Dati due sistemi:
			\[
				S_1
				\begin{cases}
					\dot{\vec{x}}(t) = \vec{A}\vec{x}(t) + \vec{B}\vec{u}(t) \\
					\vec{y}(t) = \vec{C}\vec{x}(t)
				\end{cases}
				\qquad\qquad
				S_2
				\begin{cases}
					\dot{\bar{\vec{x}}}(t) = \bar{\vec{A}}\bar{\vec{x}}(t) + \bar{\vec{B}}\bar{\vec{u}}(t) \\
					\bar{\vec{y}}(t) = \bar{\vec{C}}\bar{\vec{x}}(t)
				\end{cases}
			\]
			dove: $ \ \bar{\vec{A}} = \vec{A} - \vec{K}_o \vec{C}, \qquad \bar{\vec{B}} = \begin{bmatrix}
			\vec{B} & \vec{K}_o
			\end{bmatrix}, \qquad \bar{\vec{C}} = \vec{C}, \qquad \bar{\vec{u}}(t) = \begin{bmatrix}
			\vec{u}(t) \\ \vec{y}(t)
			\end{bmatrix}  $\\
			$ \vec{K}_o \in \R^{n\times p} $ è un \textbf{osservatore (o stimatore) asintotico} del sistema $ S_1 $ se vale
			\[
				\lim_{t\rightarrow \infty} \| \vec{x}(t) - \bar{\vec{x}}(t) \| = 0
			\]
			per tutte le possibili funzioni $ \vec{u}(t)\ $ e $\ \vec{x}(\vec{0}), \bar{\vec{x}}(\vec{0}) $.\\
			L'\textbf{equazione di stato} dell'osservatore asintotico è
			\[
				\dot{\bar{\vec{x}}}(t) = \bar{\vec{A}}\bar{\vec{x}}(t) + \bar{\vec{B}}\bar{\vec{u}}(t) + 
				\vec{K}_o(\vec{y}(t) - \bar{\vec{y}}(t))
			\]
		\end{defn}
		
		\begin{prop}
			Il sistema $ S_2 $ è \textbf{stimatore asintotico} di $ S_1 $ sse la matrice $ \bar{\vec{A}} = \vec{A} - \vec{K}_o\vec{C} $ ha \textit{tutti autovalori} a \textbf{parte reale negativa}.
		\end{prop}
	
		
\end{document}