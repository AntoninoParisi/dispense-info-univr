\documentclass[a4paper]{article}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{palatino}
\usepackage{frontespizio}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{empheq}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{caption}
\captionsetup[figure]{labelfont={bf},name={Fig.},labelsep=period}
\usepackage[left=1.5cm, right=1.5cm, top=3cm]{geometry}
\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{cleveref}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\nouppercase{\leftmark}}
\rhead{\nouppercase{\rightmark}}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.4pt}

\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	hidelinks, 
	colorlinks = true,
	linkcolor = black,
}

\newcommand{\numberset}{\mathbb}
\newcommand{\R}{\numberset{R}}
\newcommand{\C}{\numberset{C}}
\renewcommand{\vec}{\bm}

\theoremstyle{definition}
\newtheorem{exmp}{Esempio}[section]
\newtheorem{defn}{Definizione}[subsection]

\usetikzlibrary{decorations.pathreplacing}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture,baseline=(#1.base)]
	\node (#1) {\strut};}

\tikzstyle{syst} = [rectangle, 
rounded corners, minimum width=2cm, minimum height=2cm,text centered, draw=black]

\newcommand\undermat[2]{%
	\makebox[0pt][l]{$\smash{\underbrace{\phantom{%
					\begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}


\usepackage[OT1]{eulervm}

\begin{document}
	\begin{frontespizio}
		\Universita{Verona}
		\Dipartimento{Informatica}
		\Scuola{Laurea Magistrale in Ingegneria e Scienze Informatiche}
		\Titoletto{}
		\Titolo{Sistemi Dinamici}
		\Sottotitolo{Riassunto dei principali argomenti}
		\Candidato[VR424987]{Danzi Matteo}
		\Annoaccademico{2017/2018}
		\NCandidato{Autore}
	\end{frontespizio}
	
	\tableofcontents
	
	\newpage
	
	\section{Introduzione}
	
	\subsection{Sistema}
		\begin{defn}
			Un sistema è un ente fisico, tipicamente formato da diverse componenti tra
			loro interagenti, che risponde a sollecitazioni esterne producendo un determinato
			comportamento.
		\end{defn}
		
		\begin{exmp}
			Un circuito elettrico costituito da componenti quali resistori, capacitori, induttori, diodi, generatori di corrente e tensione, ecc., costituisce un semplice esempio di sistema dinamico. Il comportamento del sistema può venire descritto dal valore dei segnali di tensione e di corrente nei rami del circuito. Le sollecitazioni che agiscono sul sistema sono le tensioni e le correnti applicate dai generatori, che
			possono essere imposte dall'esterno.
		\end{exmp}
		
	\subsection{Descrizione ingresso-uscita}
		Le grandezze alla base di una descrizione IU sono le \textit{cause esterne} al sistema e gli
		\textit{effetti}. Le cause esterne sono delle grandezze che si generano al di fuori del sistema;
		la loro evoluzione influenza il comportamento del sistema ma non dipende da esso.
		Gli effetti invece sono delle grandezze la cui evoluzione dipende dalle cause esterne
		al sistema e dalla natura del sistema stesso. Di solito si usa la convenzione di definire
		come \textit{ingressi} al sistema le cause esterne, e come \textit{uscite} gli effetti. In generale su un
		sistema possono agire più ingressi così come più di una possono essere le grandezze
		in uscita. \\
		La classica rappresentazione grafica di un sistema per il quale siano stati
		individuati ingressi e uscite è quella mostrata in \hyperref[fig:io]{\textbf{Fig. 1.}} dove può venire
		considerato come un operatore che assegna uno specifico andamento alle grandezze
		in uscita in corrispondenza ad ogni possibile andamento degli ingressi.
		
		
		\begin{figure}[h!]
			\centering
			\begin{tikzpicture}[>=latex]
				\node[syst, rounded corners=false] (a) {S};
				\draw[->] ($ (a.west) +(-.75,.55) $) -- ($ (a.west) +(0,.55) $);
				\draw[->] ($ (a.west) +(-.75,-.55) $) -- ($ (a.west) +(0,-.55) $);
				\draw[->] ($ (a.east) +(0,.55) $) -- ($ (a.east) +(.75,.55) $);
				\draw[->] ($ (a.east) +(0,-.55) $) -- ($ (a.east) +(.75,-.55) $);
				\node (b) at ($ (a.west) +(-1.5,0) $) {$ \begin{matrix}
					u_1(t) \\
					\vdots \\
					u_r(t)
					\end{matrix} $};
				\node (c) at ($ (a.east) +(1.5,0) $) {$ \begin{matrix}
					y_1(t) \\
					\vdots \\
					y_p(t)
					\end{matrix} $};
				
				\node at ($ (a.south) +(0,-.5) $) {sistema}; 
				\node[align=center] at ($ (b.south) +(0,-.5) $) {ingressi\\ (cause)};
				\node[align=center] at ($ (c.south) +(0,-.5) $) {uscite\\ (effetti)};
			\end{tikzpicture}
			\caption{Descrizione in ingresso-uscita }
			\label{fig:io}
		\end{figure}
		
		Di solito su usa la convenzione di indicare con
		\[
			u(t) = \big[\ u_1(t)\ \dots\ u_r(t)\ \big]^T \ \in\ \R^r
		\]
		il vettore degli ingressi e con
		\[
			y(t) = \big[\ y_1(t)\ \dots\ y_p(t)\ \big]^T \ \in\ \R^p
		\]
		il vettore delle uscite.
	
	\subsection{Descrizione in variabili di stato}
		È facile rendersi conto che in generale l'uscita di un sistema in un
		certo istante di tempo non dipende dal solo ingresso al tempo, ma dipende anche
		dall'evoluzione precedente del sistema.
		
		Di questo fatto è possibile tenere conto introducendo una grandezza intermedia tra ingressi e uscite, chiamata \textit{stato} del sistema. Lo stato del sistema gode della	proprietà di concentrare in sè l'informazione sul passato e sul presente del sistema.
		Così come le grandezze di ingresso e uscita, anche lo stato è in generale una
		grandezza vettoriale e viene indicato mediante un vettore di stato
		
		\[
			x(t) = \big[\ x_1(t)\ \dots\ x_n(t)\ \big]^T \ \in\ \R^n
		\]
		dove il numero di componenti del vettore di stato si indica con e viene detto \textit{ordine}
		del sistema.
		
		\begin{defn}
			Lo stato di un sistema all'istante di tempo è la grandezza che contiene l'informazione necessaria per determinare univocamente l'andamento dell'uscita $ y(t) $, per ogni $ t \geq t_0 $, sulla base della conoscenza dell'andamento dell'ingresso $ u(t) $, per $ t \geq t_0 $ e appunto dello stato in $ t_0 $.
		\end{defn}
		
		\begin{figure}[t!]
			\centering
			\begin{tikzpicture}[>=latex]
			\node[syst, rounded corners=false, minimum width=3cm, minimum height=2.5cm] (a) {$ \vdots $};
			
			\node[draw, rectangle, minimum width=0.5cm, minimum height=1.5cm] (a1) at ($ (a) +(-1,0) $) {}; 
			\node[draw, rectangle, minimum width=0.5cm, minimum height=1.5cm] (a2) at ($ (a) +(1,0) $) {};
			
			
			\draw[->] ($ (a1.east) +(0,.55) $) -- ($ (a2.west) +(0,.55) $) node[midway, above] {$ x_1(t) $};
			\draw[->] ($ (a1.east) +(0,-.55) $) -- ($ (a2.west) +(0,-.55) $) node[midway, below] {$ x_n(t) $};
			\draw[->] ($ (a.west) +(-.75,.55) $) -- ($ (a1.west) +(0,.55) $);
			\draw[->] ($ (a.west) +(-.75,-.55) $) -- ($ (a1.west) +(0,-.55) $);
			\draw[->] ($ (a2.east) +(0,.55) $) -- ($ (a.east) +(.75,.55) $);
			\draw[->] ($ (a2.east) +(0,-.55) $) -- ($ (a.east) +(.75,-.55) $);
			\node (b) at ($ (a.west) +(-1.5,0) $) {$ \begin{matrix}
				u_1(t) \\
				\vdots \\
				u_r(t)
				\end{matrix} $};
			\node (c) at ($ (a.east) +(1.5,0) $) {$ \begin{matrix}
				y_1(t) \\
				\vdots \\
				y_p(t)
				\end{matrix} $};
			
			\node at ($ (a.south) +(0,-.5) $) {stati}; 
			\node at ($ (b.south) +(0,-.5) $) {ingressi};
			\node at ($ (c.south) +(0,-.5) $) {uscite};
			\end{tikzpicture}
			\caption{Descrizione in variabili di stato }
			\label{fig:state}
		\end{figure}
		
	\section{Modello matematico di un sistema}
		L'obiettivo dell'Analisi dei Sistemi consiste nel studiare il legame esistente tra gli ingressi e le uscite di un sistema e/o tra gli stati, gli ingressi e le uscite del sistema.
		In altri termini, risolvere un problema di analisi significa capire, dati certi segnali in ingresso al sistema, come evolveranno gli stati e le uscite di tale sistema.\\
		Questo rende necessaria la definizione di un modello matematico che descriva in maniera quantitativa il comportamento del sistema allo studio, ossia fornisca una descrizione matematica esatta del legame tra ingressi (stati) e uscite.
		A seconda del tipo di descrizione che si vuole dare al sistema (IU o VS) è necessario formulare due diversi tipi di modello.
		
		\subsection{Modello ingresso-uscita}
			Il modello IU per un sistema MIMO, ossia un sistema con $ p $ uscite e $ r $ ingressi, è espresso mediante $ p $ equazioni differenziali del tipo:
			\[
				\begin{cases}
					h_1 = \left(\underbrace{ y_1(t), \dots, y_1^{(n_1)}(t) }_{\textup{uscita 1}}, 
						\underbrace{ u_1(t), \dots, u_1^{(m_{1,1})}(t) }_{\textup{ingresso 1}}, \dots,
						\underbrace{ u_r(t), \dots, u_r^{(m_{1,r})}(t) }_{\textup{ingresso }r}, t
					\right) = 0\\ \\
					h_2 = \left(\underbrace{ y_2(t), \dots, y_2^{(n_2)}(t) }_{\textup{uscita 2}}, 
						\underbrace{ u_1(t), \dots, u_1^{(m_{2,1})}(t) }_{\textup{ingresso 1}}, \dots,
						\underbrace{ u_r(t), \dots, u_r^{(m_{2,r})}(t) }_{\textup{ingresso }r}, t
					\right) = 0\\
					\qquad\qquad \vdots \\
					h_p = \left(\underbrace{ y_p(t), \dots, y_p^{(n_p)}(t) }_{\textup{uscita }p}, 
						\underbrace{ u_1(t), \dots, u_1^{(m_{p,1})}(t) }_{\textup{ingresso 1}}, \dots,
						\underbrace{ u_r(t), \dots, u_r^{(m_{p,r})}(t) }_{\textup{ingresso }r}, t
					\right) = 0\\
				\end{cases}
			\]
			
			dove:
			\begin{itemize}
				\item $ h_i,\ i = 1,\dots, p $ sono funzioni di più parametri che dipendono dal particolare
				sistema allo studio,
				\item $ n_i $ è il grado massimo di derivazione della $ i$-esima componente dell'uscita $ y_i(t) $,
				\item $ m_i $ è il grado massimo di derivazione della $ i$-esima componente dell'ingresso $ u_i(t) $.
			\end{itemize}
			
		\subsection{Modello in variabili di stato}
			Il modello in VS per un sistema MIMO con $ r $ ingressi e $ p $ uscite ha invece una
			struttura del tipo
			\[
				\begin{cases}
					\dot{x}_1 (t) = f_1(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t) \\
					\qquad \quad\vdots \\
					\dot{x}_n (t) = f_n(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t) \\ \\
					y_1(t) = g_1(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t) \\
					\qquad \quad\vdots \\
					y_p(t) = g_p(x_1(t),\dots, x_n(t), u_1(t),\dots, u_r(t),t)
				\end{cases}
			\]
			che riscritto in forma matriciale diviene
			\[
				\begin{cases}
					\dot{\vec{x}}(t) = \vec{f}(\vec{x}(t),\vec{u}(t),t) \\
					\vec{y}(t) = \vec{g}(\vec{x}(t),\vec{u}(t),t)
				\end{cases}
			\]
			L'equazione di stato è pertanto un sistema di $ n $ equazioni differenziali del primo
			ordine, a prescindere dal fatto che il sistema sia SISO o MIMO. La \textit{trasformazione
			in uscita} è invece una equazione algebrica, scalare o vettoriale a seconda del numero
			delle variabili in uscita.
			La rappresentazione schematica che si può dare di un modello in VS è pertanto
			quella riportata in \hyperref[fig:VS]{\textbf{Fig. 3.}}
			\begin{figure}[h!]
				\centering
				\begin{tikzpicture}[>=latex]
					\node[draw, rectangle,minimum height=1cm] (a) {$ \dot{\vec{x}}(t) = \vec{f}(\vec{x}(t),\vec{u}(t),t) $};
					\node[draw, rectangle,minimum height=1cm] (b) at (5,0) {$ \vec{y}(t) = \vec{g}(\vec{x}(t),\vec{u}(t),t) $};
					
					\draw[->] ($ (a.west) +(-1,0) $) -- (a.west) node[midway, above] {$ \vec{u}(t) $};
					\draw[->] ($ (a.west) +(-.75,0) $) -- ++(0,-1) -- ++(5,0) -- ++(0,0.75) -- ($ (b.west) +(0,-0.25) $);
					\draw[->] (a.east) -- (b.west) node[midway, above] {$ \vec{x}(t) $};
					\draw[->] (b.east) -- ++(1,0) node[midway, above] {$ \vec{y}(t) $};
				\end{tikzpicture}
				\caption{Rappresentazione schematica di un modello in variabili di stato}
				\label{fig:VS}
			\end{figure}
			
	\section{Proprietà dei sistemi}
		\subsection{Sistemi dinamici o istantanei}
			La prima importante distinzione che si può fare è tra sistemi istantanei e sistemi
			dinamici.
			
			\begin{defn}
				Un sistema è detto
				\begin{itemize}
					\item \textit{Istantaneo}: se il valore $ \vec{y}(t) $ assunto dall'uscita al tempo $ t $ dipende solo dal valore $ \vec{u}(t) $ assunto dall'ingresso al tempo $ t $;
					\item \textit{Dinamico}: in caso contrario.
				\end{itemize}
			\end{defn}
			
			Condizione necessaria e sufficiente affinché un sistema MIMO con $ r $ ingressi e $ p $ uscite sia istantaneo è che il legame \textbf{IU} sia espresso da un insieme di equazioni della forma:
			\[
				\begin{cases}
					h_1 (y_1(t),u_1(t),u_2(t),\dots, u_r(t),t) = 0 \\
					h_2 (y_2(t),u_1(t),u_2(t),\dots, u_r(t),t) = 0 \\
					\vdots \\
					h_p (y_p(t),u_1(t),u_2(t),\dots, u_r(t),t) = 0.
				\end{cases}
			\]
			
			Condizione necessaria e sufficiente affinché un sistema sia istantaneo è che il modello in \textbf{VS} abbia ordine zero ovvero che \textit{non esista il vettore di stato}.
			
		\subsection{Sistemi lineari o non lineari}
			Una delle proprietà fondamentali di cui gode un'ampia classe di sistemi (o più precisamente di modelli) è la linearità. È proprio su questa classe di sistemi che focalizzeremo la nostra attenzione in questo volume. L'importanza dei sistemi lineari deriva
			da una serie di considerazioni pratiche.\\
			La prima è che tali sistemi sono facili da studiare. Per essi sono state proposte
			efficienti tecniche di analisi e di sintesi, non più applicabili se la linearità viene meno.
			In secondo luogo, un modello lineare si rivela una buona approssimazione del
			comportamento di numerosi sistemi reali purchè questi siano sottoposti a piccoli
			ingressi.
			
			\begin{defn}
				Un sistema è detto
				\begin{itemize}
					\item \textit{Lineare}: se per esso vale il principio di sovrapposizione degli effetti. Ciò significa che se il sistema risponde alla causa $ c_1 $ con l'effetto $ e_1 $ e alla causa $ c_2 $ con l'effetto $ e_2 $, allora la risposta del sistema alla causa $ ac_1 + bc_2 $ è $ ae_1 + be_2 $ qualunque siano i valori assunti dalle costanti $ a $ e $ b $.
					Il seguente schema riassume tale proprietà:
					\[
						\begin{rcases*}
							causa\ c_1\ \rightsquigarrow\ effetto\ e_1 \\
							causa\ c_2\ \rightsquigarrow\ effetto\ e_2
						\end{rcases*}
						\
						\Longrightarrow
						\
						causa\ (ac_1 + bc_2)\ \rightsquigarrow\ effetto\ (ae_1 + be_2);
					\]
					\item \textit{Non lineare}: in caso contrario.
				\end{itemize}
			\end{defn}
			
		Condizione necessaria e sufficiente affinché un sistema sia lineare è che il legame \textbf{IU} sia espresso da una equazione differenziale	lineare, cioè per un sistema SISO:
		\[
			a_0(t) y(t) + a_1(t)\dot{y}(t) + \cdots + a_n(t) y^{(n)}(t)\ =\
			b_0(t) u(t) + b_1(t)\dot{u}(t) + \cdots + b_m(t)u^{(m)}(t)
		\]
		Un sistema MIMO invece è lineare se e solo se ciascuna delle funzioni $ h_i,\ i = 1,\dots , p $, esprime una combinazione lineare tra la $ i$-esima componente dell'uscita e le sue $ n_i $ derivate e le variabili di ingresso con le loro derivate.
		
		\bigskip
		
		Condizione necessaria e sufficiente affinché un
		sistema sia lineare è che nel modello in \textbf{VS} sia l'equazione di stato che la trasformazione di uscita siano equazioni lineari:
		\[
			\begin{cases}
				\dot{x}_1(t) = a_{1,1}(t) x_1(t) + \cdots + a_{1,n}(t) x_n(t) + b_{1,1} u_1(t) + \cdots + b_{1,r} u_r(t) \\
				\qquad\vdots \\
				\dot{x}_n(t) = a_{n,1}(t) x_1(t) + \cdots + a_{n,n}(t) x_n(t) + b_{n,1} u_1(t) + \cdots + b_{n,r} u_r(t) \\ \\
				y_1(t) = c_{1,1}(t) x_1(t) + \cdots + c_{1,n}(t) x_n(t) + d_{1,1} u_1(t) + \cdots + d_{1,r} u_r(t) \\
				\qquad\vdots \\
				y_p(t) = c_{p,1}(t) x_1(t) + \cdots + c_{p,n}(t) x_n(t) + d_{p,1} u_1(t) + \cdots + d_{p,r} u_r(t) \\
			\end{cases}
		\]
		ovvero
		\[
			\begin{cases}
				\vec{\dot{x}}(t) = \vec{A}(t)\vec{x}(t) + \vec{B}(t)\vec{u}(t) \\
				\vec{y}(t) = \vec{C}(t)\vec{x}(t) + \vec{D}(t)\vec{u}(t)
			\end{cases}
		\]
		dove
		
		\begin{align*}
			\vec{A}(t) &= \{a_{i,j} (t) \} \ \textup{matrice} \ n\times n;\qquad
			&\vec{B}(t) = \{b_{i,j}(t) \} \ \textup{matrice} \ n\times r; \\
			\vec{C}(t) &= \{c_{i,j} (t) \} \ \textup{matrice} \ p\times n;\qquad
			&\vec{D}(t) = \{d_{i,j}(t) \} \ \textup{matrice} \ p\times r; 
		\end{align*}
		
	\subsection{Tempo-invarianza}
		Un'altra importante proprietà di cui gode un'ampia classe di sistemi è la \textit{tempo-invarianza} o stazionarietà. In particolare, in questo testo ci occuperemo proprio dell'analisi dei sistemi
		lineari e stazionari.
		
		\begin{defn}
			Un sistema è detto
			\begin{itemize}
				\item \textit{Tempo-invariante}: se per esso vale il principio di traslazione causa-effetto nel tempo, cioè se il sistema risponde sempre con lo stesso effetto ad una data causa, a prescindere dall'istante di tempo in cui tale causa agisca
				sul sistema.\\
				Il seguente schema riassume tale proprietà:
				\[
					\textup{causa}\ c(t)\ \rightsquigarrow \ \textup{effetto}\ e(t)\ \
					\Longrightarrow\ \ \textup{causa}\ c(t - T)\  \rightsquigarrow \ \textup{effetto}\ e(t-T);
				\]
				\item \textit{Tempo-variante}: in caso contrario.
			\end{itemize}
			
			La \hyperref[fig:time]{\textbf{Fig. 4.}} mostra il comportamento tipico di un sistema lineare sollecitato
			dalla stessa causa applicata in due diversi intervalli di tempo, ossia a partire da $ t = 0 $
			e a partire da $ t = T $: nei due casi l'effetto risultante è analogo ma ha semplicemente
			origine da istanti di tempo che differiscono tra di loro proprio di una quantità pari a $ T $.
			
			\begin{figure}[h!]
				\centering
				\begin{tikzpicture}[>=latex]
					\draw[->] (0,-.25) -- ++(0,2) node[left] {$ e(t) $}; \draw[->] (-.25,0) -- ++(3,0) node[below] {$ t $};
					\draw[->] (5,-.25) -- ++(0,2) node[left] {$ e(t - T) $}; \draw[->] (4.75,0) -- ++(3,0) node[below] {$ t $};
					\draw[->] (0, 2.75) -- ++(0,2) node[left] {$ c(t) $}; \draw[->] (-.25,3) -- ++(3,0) node[below] {$ t $};
					\draw[->] (5,2.75) -- ++(0,2) node[left] {$ c(t - T) $}; \draw[->] (4.75,3) -- ++(3,0) node[below] {$ t $};
					
					\draw[thick] (-.25,3) -- ++(.25,0) -- ++(0,1) .. controls (0.25,4.7) and (.75,2.5)
					.. (2,4);
					\draw[thick] (4.75,3) -- ++(1,0) -- ++(0,1) .. controls (6 ,4.7) and (6.55,2.5)
					.. (7.75,4);
					
					\draw[thick] (-.25,0) .. controls (1,0) and (1, .75) .. (2, 1.5);
					\draw[thick] (4.75,0) -- (5.5,0);
					\draw[thick,xshift=5.75cm] (-.25,0) .. controls (1,0) and (1, .75) .. (2, 1.5);
					
					\draw[<->] (5,0.25) -- (5.75,0.25) node[midway, above] {$ T $};
					\draw (5.75,0) -- (5.75,.5);
					\draw[<->,yshift=3cm] (5,0.25) -- (5.75,0.25) node[midway, above] {$ T $};
				\end{tikzpicture}
				\caption{Traslazione causa-effetto nel tempo}
				\label{fig:time}
			\end{figure}
		\end{defn}
		
		Condizione necessaria e sufficiente affinché un sistema sia stazionario è che il legame \textbf{IU} non dipenda esplicitamente dal tempo, cioè per un sistema SISO:
		\[
			h \left( y(t), \dot{y}(t), \dots, y^{(n)}(t), u(t), \dot{u}(t),\dots, u^{(m)}(t) \right) = 0
		\]
		che nel caso dei sistemi lineari si riduce a una equazione differenziale lineare a
		coefficienti costanti:
		\[
			a_0 y(t) + a_1\dot{y}(t) + \cdots + a_n y^{(n)}(t) = 
			b_0 u(t) + b_1\dot{u}(t) + \cdots + b_n u^{(m)}(t).
		\]
		
		Condizione necessaria e sufficiente affinché un	sistema sia stazionario è che nel modello in \textbf{VS} l'equazione di stato e la trasformazione di uscita non dipendano esplicitamente dal tempo:
		\[
			\begin{cases}
				\vec{\dot{x}}(t)\ =\vec{f}(\vec{x}(t),\vec{u}(t)) \\
				\vec{y}(t)\ =\vec{g}(\vec{x}(t),\vec{u}(t)) 
			\end{cases}
		\]
		che nel caso dei sistemi lineari si riduce a
		\[
			\begin{cases}
				\vec{\dot{x}}(t)\ =\vec{A}\vec{x}(t)+ \vec{B}\vec{u}(t)  \\
				\vec{y}(t)\ =\vec{C}\vec{x}(t)+ \vec{D}\vec{u}(t) 
			\end{cases}
		\]
		dove $ \vec{A}, \vec{B}, \vec{C} $ e $ \vec{D} $ sono matrici costanti.
		
		\newpage
		
	\section{Analisi nel dominio del tempo delle rappresentazioni in variabili di stato}
		Un sistema lineare e stazionario di ordine $ n $, con $ r $ ingressi e $ p $ uscite, ha la seguente rappresentazione in VS:
		\[
			\begin{cases}
				\vec{\dot{x}}(t)\ =\vec{A}\vec{x}(t)+ \vec{B}\vec{u}(t)  \\
				\vec{y}(t)\ =\vec{C}\vec{x}(t)+ \vec{D}\vec{u}(t) 
			\end{cases}
		\]
	
		Il problema fondamentale dell'analisi dei sistemi per un tale sistema consiste nel
		determinare l'andamento dello stato $ \vec{x}(t) $ e dell'uscita $ \vec{y}(t) $ per $ t \geq t_0 $ noto:
		\begin{itemize}
			\item il valore dello stato iniziale $ \vec{x}(t_0) $;
			\item l'andamento dell'ingresso $ \vec{u}(t) $ per $ t\geq t_0 $.
		\end{itemize}

		\subsection{Matrice di transizione di stato}
			Data una matrice quadrata $ \vec{A} $ il suo esponenziale è la matrice
			\[
				e^{\vec{A}} = \vec{I} + \vec{A} + \dfrac{\vec{A}^2}{2!} + \dfrac{\vec{A}^2}{3!} + \cdots =
				\sum_{k = 0}^{\infty} \dfrac{\vec{A}^k t^k}{k!}
			\]
			La matrice di transizione dello stato $ e^{\vec{A}} $ è una particolare matrice esponenziale i cui elementi sono funzioni del tempo di dimensione $ n\times n $ e anche $ \vec{A} $ è $ n\times n $
			\[
				e^{\vec{A}t} = \sum_{k=0}^{\infty} \dfrac{\vec{A}^k t^k}{k!}
			\]
			Se $ \vec{A} $ è una matrice \textbf{diagonale} di dimensione $ n\times n $
			\[
				\vec{A} = 
				\begin{bmatrix}
					\lambda_1 & 0 & \cdots & 0 \\
					0 & \lambda_2 & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & \lambda_n 
				\end{bmatrix}
				\qquad
				\textup{vale}
				\qquad
				e^{\vec{A}t} =
				\begin{bmatrix}
					e^{\lambda_1 t} & 0 & \cdots & 0 \\
					0 & e^{\lambda_2 t} & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & e^{\lambda_n t}
				\end{bmatrix}
			\]
			
			
		\subsection{Sviluppo di Sylvester}
			Ci si pone ora il problema di determinare l'espressione analitica della matrice di transizione dello stato $ e^{\vec{A}t} $ senza dover necessariamente calcolare la serie infinita che la
			definisce. Tale procedura si basa sullo sviluppo di Sylvester, esiste una seconda procedura, basata sul passaggio alla forma diagonale o alla forma di Jordan e una terza procedura, basata sull'uso delle
			trasformate di Laplace.
			
			\begin{defn}
				Se $ \vec{A} $ è una matrice di dimensione $ n\times n $, la corrispondente matrice di transizione dello stato $ e^{\vec{A}t} $ può essere scritta come:
				\[
					e^{\vec{A}t} = \sum_{i = 0}^{n - 1} \beta_i (t)\vec{A}^i = 
					\beta_0 (t)\vec{I} + \beta_1(t)\vec{A} + \cdots + \beta_{n-1}(t)\vec{A}^{n-1}
				\]
				dove i coefficienti dello sviluppo $ \beta_i(t) $ sono opportune funzioni scalari nel tempo.			
			\end{defn}
			I coefficienti dello sviluppo di Sylvester possono venire determinati risolvendo un
			sistema di equazioni lineari. Esistono sostanzialmente tre diversi casi che vengono discussi di seguito.
			
		\subsubsection{Autovalori di molteplicità unitaria}
			Se la matrice $ \vec{A} $ ha autovalori tutti distinti $ \lambda_1, \lambda_2,\dots, \lambda_n $, le $ n $ funzioni incognite $ \beta_i(t) $ si ricavano risolvendo il seguente sistema di $ n $ equazioni (tante equazioni quanti sono gli autovalori):
			\[
				\begin{cases}
					\beta_0 (t) + \lambda_1\beta_1 (t) + \lambda_1^2\beta_2 (t) + \cdots + \lambda_1^{n-1} \beta_{n-1}(t) = e^{\lambda_1 t} \\
					\beta_0 (t) + \lambda_2\beta_1 (t) + \lambda_2^2\beta_2 (t) + \cdots + \lambda_2^{n-1} \beta_{n-1}(t) = e^{\lambda_2 t} \\
					\qquad \vdots \qquad \vdots \\
					\beta_0 (t) + \lambda_n\beta_1 (t) + \lambda_n^2\beta_2 (t) + \cdots + \lambda_n^{n-1} \beta_{n-1}(t) = e^{\lambda_n t} \\
				\end{cases}
			\]
			ovvero risolvendo il sistema di equazioni lineari
			\[
				\vec{V}\vec{\beta} = \vec{\eta}
			\]
			dove
			\begin{itemize}
				\item $ \beta = \begin{bmatrix}
					\beta_0(t) & \beta_1(t) & \cdots& \beta_{n-1}(t)
				\end{bmatrix}^T $ è il vettore delle incognite
				\item La matrice dei coefficienti vale
				\[
					\vec{V} = 
					\begin{bmatrix}
						1 & \lambda_1 & \cdots & \lambda_1^{n-1} \\
						1 & \lambda_2 & \cdots & \lambda_2^{n-1} \\
						\vdots & \vdots & \ddots & \vdots \\
						1 & \lambda_n & \cdots & \lambda_n^{n-1} \\
					\end{bmatrix}
				\]
				\item $ \eta = \begin{bmatrix}
				e^{\lambda_1 t} & e^{\lambda_2 t} & \cdots & e^{\lambda_n t}
				\end{bmatrix}^T $ è il vettore dei termini noti.
			\end{itemize}
		
			Viene detta \textbf{modo della matrice} $ \vec{A} $ associato all'autovalore $ \lambda $ la generica componente $ e^{\lambda t} $ (una funzione nel tempo) del vettore  $ \vec{\eta} $.
			
		\subsubsection{Autovalori di molteplicità non unitaria}
			Se la matrice $ \vec{A} $ ha autovlori di molteplicità non unitaria, si costruisce un sistema in cui ad ogni autovalore $ \lambda $ di molteplicità $ \nu $ corrispondono $ \nu $ equazioni della forma:
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1} \beta_{n-1} (t) &= e^{\lambda t} \\
				\dfrac{d}{d\lambda}\left( \beta_0(t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1}\beta_{n-1}(t) \right) &= \dfrac{d}{d\lambda}e^{\lambda t} \\
				\qquad \vdots \qquad \qquad \qquad \qquad & \qquad \vdots \\
				\dfrac{d^{\nu -1}}{d\lambda^{\nu -1}}\left( \beta_0(t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1}\beta_{n-1}(t) \right) &= \dfrac{d^{\nu -1}}{d\lambda^{\nu -1}}e^{\lambda t} \\
			\end{empheq}
			ovvero
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + \lambda\beta_1 (t) + \cdots + \lambda^{n-1} \beta_{n-1} (t) &= e^{\lambda t} \\
				\beta_1(t) + 2\lambda\beta_2 (t) + \cdots + (n - 1)\lambda^{n-2}\beta_{n-1}(t) &= t e^{\lambda t} \\
				\qquad \vdots \qquad \qquad \qquad \qquad & \qquad \vdots \\
				\dfrac{(\nu -1)!}{0!} \beta_{\nu - 1}(t) + \cdots + 
				\dfrac{(n-1)!}{(n - \nu)!}\lambda^{n-\nu}\beta_{n-1}(t) &= t^{\nu - 1} e^{\lambda t} .\\
			\end{empheq}
			
			Anche in tal caso è possibile scrivere un sistema lineare in cui ad ogni autovalore $ \lambda $ i molteplicità $ \nu $ sono associate $ \nu $ righe della matrice dei coefficienti $ \vec{V} $:
			\[
				\begin{bmatrix}
					1 & \lambda & \lambda^2 & \cdots & \lambda^{\nu - 1} & \cdots & \lambda^{n - 1} \\
					0 & 1 & 2\lambda & \cdots & (\nu - 1)\lambda^{\nu - 2} & \cdots & (n - 1)\lambda^{n-2} \\
					\vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
					0 & 0 & 0 & \cdots & (\nu - 1)! & \cdots & \dfrac{(n-1)!}{(n - \nu)!}\lambda^{n - \nu}
				\end{bmatrix}
			\]
			e $ \nu $ righe del vettore dei termini noti $ \vec{\eta} = 
			\begin{bmatrix}
				e^{\lambda t} & te^{\lambda t} & \cdots & t^{\nu - 1}e^{\lambda t}
			\end{bmatrix}^T $.
			
		\subsubsection{Autovalori complessi}
			Anche nel caso in cui vi siano autovalori complessi è possibile determinare i
			coefficienti dello sviluppo di Sylvester come sopra indicato.\\
			Per evitare, tuttavia, di lavorare con numeri complessi conviene modificare la
			procedura per il calcolo dei coefficienti $ \beta $ come segue (tratteremo solo il caso di autovalori di molteplicità unitaria per semplicità). Supponiamo che fra gli $ n $ autovalori della matrice ve ne siano 2 complessi e coniugati $ \lambda, \lambda' = \alpha \pm j \omega $.
			
			In tal caso nel sistema di equazioni dovrebbero comparire le due equazioni
			
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + \lambda_1\beta_1 (t) + \lambda_1^2\beta_2 (t) + \cdots & + \lambda_1^{n-1} \beta_{n-1}(t) &= e^{\lambda_1 t} &= e^{\alpha t}e^{j\omega t}\\
				\beta_0 (t) + \lambda'\beta_1 (t) + (\lambda' )^2\beta_2 (t) + \cdots & + (\lambda')^{n-1} \beta_{n-1}(t) &= e^{\lambda' t} &=e^{\alpha t}e^{j\omega t}
			\end{empheq}
			Possiamo tuttavia sostituire queste due equazioni con due equazioni equivalenti
			in cui non compaiono termini complessi:
			\begin{empheq}[left={\empheqlbrace}]{alignat*=2}
				\beta_0 (t) + Re(\lambda)\beta_1 (t) + Re(\lambda^2)\beta_2 (t) + \cdots & + Re(\lambda^{n-1}) \beta_{n-1}(t) &= e^{\lambda_1 t} cos(\omega t)\\
				Im(\lambda)\beta_1 (t) + Im(\lambda )^2\beta_2 (t) + \cdots & + Im(\lambda^{n-1}) \beta_{n-1}(t) &= e^{\lambda t} sin(\omega t)
			\end{empheq}
			dove $ Re $ e $ Im $ indicano la parte reale e immaginaria di un numero complesso. In particolare dunque vale $ Re(\lambda) = \alpha $ e $ Im(\lambda) = \omega $
			
			
	\subsection{Formula di Lagrange}
		Possiamo finalmente dimostrare un importante risultato che determina la soluzione
		al problema di analisi per i sistemi MIMO precedentemente enunciato. Tale risultato
		è noto con il nome di formula di Lagrange.
		\begin{defn}
			La soluzione del sistema 
			\[
			\begin{cases}
			\vec{\dot{x}}(t)\ =\vec{A}\vec{x}(t)+ \vec{B}\vec{u}(t)  \\
			\vec{y}(t)\ =\vec{C}\vec{x}(t)+ \vec{D}\vec{u}(t) 
			\end{cases}
			\]
			con stato iniziale $ \vec{x}(t_0) $ e andamento dell'ingresso $ \vec{u}(t) $ (per $ t\geq t_0 $) vale per $ t\geq t_0 $:
			\[
				\begin{cases}
					\vec{x}(t) = e^{\vec{A}(t - t_0)}\vec{x}(t_0) + 
					\displaystyle\int_{t_0}^{t} e^{\vec{A}(t-\tau)}\vec{B}\vec{u}(\tau)d\tau \\ \\
					\vec{y}(t) = \vec{C}e^{\vec{A}(t - t_0)}\vec{x}(t_0) + 
					\vec{C}\displaystyle\int_{t_0}^{t} \vec{B}\vec{u}(\tau)d\tau + \vec{D}\vec{u}(t)
				\end{cases}
			\]
		\end{defn}
		
		
		\subsubsection{Evoluzione libera e evoluzione forzata}
			In base al precedente risultato possiamo anche scrivere l'\textbf{evoluzione dello stato} per $ t\geq t_0 $
			come la somme di due termini:
			\[
				\vec{x}(t) = \vec{x}_l(t) + \vec{x}_f(t).
			\]
			\begin{itemize}
				\item Il termine 
				\[
					\vec{x}_l(t) = e^{\vec{A}(t-t_0)}\vec{x}(t_0)
				\]
				corrisponde all'\textit{evoluzione libera dello stato} a partire dalle condizioni iniziali $ \vec{x}(t_0) $. Si noti che $ e^{\vec{A}(t-t_0)} $ indica appunto come avviene la transizione dallo stato $ \vec{x}(t_0) $ allo stato $ \vec{x}(t) $ in assenza di contributi dovuti all'ingresso.
				\item Il termine
				\[
					\vec{x}_f(t) = \int_{t_0}^{t} e^{\vec{A}(t - \tau)}\vec{B}\vec{u}(\tau)d\tau =
					\int_{0}^{t-t_0} e^{\vec{A}\tau}\vec{B}\vec{u}(t - \tau)d\tau
				\]
				corrisponde all'\textit{evoluzione forzata dello stato} (la seconda equazione si dimostra
				per cambiamento di variabile) . Si osservi che in tale integrale il contributo di 
				$ \vec{u}(\tau) $ allo stato $ \vec{x}(t) $ è pesato tramite la funzione ponderatrice 
				$ e^{\vec{A}(t - \tau)}\vec{B} $.
			\end{itemize}
			
			Anche l'\textbf{evoluzione dell'uscita} per $ t\geq t_0 $ si può scrivere come la somma di due termini:
			\[
				\vec{y}(t) = \vec{y}_l(t) + \vec{y}_f(t).
			\]
			\begin{itemize}
				\item Il termine
				\[
					\vec{y}_l(t) = \vec{C}\vec{x}_l(t) = \vec{C}e^{\vec{A}(t-t_0)}\vec{x}(t_0)
				\]
				corrisponde all'\textit{evoluzione libera dell'uscita} a partire dalle condizioni iniziali $ \vec{y}(t_0) = \vec{C}\vec{x}(t_0) $.
				\item Il termine
				\[
					\vec{y}_f(t) = \vec{C}\vec{x}_f(t) + \vec{D}\vec{u}(t) = 
					\vec{C}\int_{t_0}^{t} \vec{B}\vec{u}(\tau)d\tau + \vec{D}\vec{u}(t)
				\]
				corrisponde all'\textit{evoluzione forzata dell'uscita}.
			\end{itemize}
		
		\subsubsection{Risposta impulsiva di una rappresentazione in VS}
		
		\subsection{Trasformazione di similitudine}
		
		\subsection{Diagonalizzazione}
		
		\subsection{Forma di Jordan}
			Si consideri una matrice $ \vec{A} $ di dimensione $ n\times n $ i cui autovalori hanno molteplicità
			non unitaria. In tal caso non vi è garanzia che esistano $ n $ autovettori linearmente indipendenti con cui costruire una matrice modale: dunque non è sempre possibile determinare una trasformazione di similitudine che porti ad una forma diagonale.\\
			Si dimostra, tuttavia, che è sempre possibile, estendendo il concetto di autovettore, determinare un insieme di $ n $ \textit{autovettori generalizzati} linearmente indipendenti.\\
			Tali vettori possono venire usati per costruire una \textit{matrice modale generalizzata} che 
			consente, per similitudine, di passare ad una matrice in \textit{forma di Jordan}, una forma
			canonica diagonale a blocchi che generalizza la forma diagonale.
			
			\begin{defn}
				Dato un numero complesso $ \lambda \in \C $ e un numero intero $ p\geq 1 $ definiamo \textbf{blocco di Jordan} di ordine $ p $ associato a $ lambda $ la matrice quadrata
				\[
					\vec{J} =
					\left.
					\begin{bmatrix}
						\lambda & 1 & 0 & \cdots & 0 & 0 \\
						0 & \lambda & 1 & \cdots & 0 & 0 \\
						0 & 0 & \lambda & \cdots & 0 & 0 \\
						\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
						0 & 0 & 0 & \cdots & \lambda & 1 \\
	 					\tikzmark{lower1}0 & 0 & 0 & \cdots & 0 & \lambda\tikzmark{lower2}
					\end{bmatrix}
					\quad
					\right\rbrace 
					\text{p}
				\]
				\begin{tikzpicture}[overlay, remember picture,decoration={brace,amplitude=5pt}]
				\draw[decorate,thick] ($ (lower2.south) +(0,-.25) $) -- ($ (lower1.south) +(0,-.25) $)
				node [midway,below=5pt] {p};
				\end{tikzpicture}
				
				\bigskip\bigskip
				
				Ogni elemento lungo la diagonale di tale matrice vale $ \lambda $, mentre ogni elemento lugo la sopra diagonale vale $ 1 $; ogni altro elemento è nullo. Dunque $ \lambda $ è un autovalore di molteplicità $ p $ del blocco $ \vec{J} $.
			\end{defn}
			Possiamo ora definire la forma canonica di Jordan.
			\begin{defn}
				Una matrice $ \vec{A} $ è detta in \textbf{forma di Jordan} se essa è una matrice diagonale a blocchi
				\[
					\vec{A} =
					\begin{bmatrix}
						\vec{J}_1 & \vec{0} & \cdots & \vec{0} \\
						0 & \vec{J}_2 & \cdots & \vec{0} \\
						\vdots & \vdots & \ddots & \vdots \\
						\vec{0} & \vec{0} & \cdots & \vec{J}_q
 					\end{bmatrix}
				\]
				In cui ogni blocco $ \vec{J}_i $ lungo la diagonale è un blocco di Jordan associato ad un autovalore $ \lambda_i $ (per $ i = 1,\dots,q $).
			\end{defn}
			
			\begin{defn}[Struttura di autovettori generalizzati]
				Sia $ \vec{A} $ una matrice $ n\times n $ e sia $ \lambda $ un autovalore di molteplicità $ \nu $ a cui corrispondono $ \mu $ autovettori linearmente indipendenti ( con $ 1\leq \mu \leq \nu $).
				A tale autovalore compete una struttura di $ \nu $ autovettori generalizzati linearmente indipendenti costituita da $ \mu $ catene:
				\[
					\begin{cases}
						\vec{v}_{p_1}^{(1)}\ \rightarrow\ \cdots \ \rightarrow\ \vec{v}_{2}^{(1)}\ \rightarrow\ \vec{v}_{1}^{(1)} \qquad & \textup{catena 1} \\ \\
						\vec{v}_{p_2}^{(2)}\ \rightarrow\ \cdots \ \rightarrow\ \vec{v}_{2}^{(2)}\ \rightarrow\ \vec{v}_{1}^{(2)} \qquad & \textup{catena 2} \\
						\qquad\qquad \vdots & \qquad \vdots \\
						\vec{v}_{p_\mu}^{(\mu)}\ \rightarrow\ \cdots \ \rightarrow\ \vec{v}_{2}^{(\mu)}\ \rightarrow\ \vec{v}_{1}^{(\mu)} \qquad & \textup{catena }\mu 
					\end{cases}
				\]
				il numero di catene $ \mu $ è detto \textbf{molteplicità geometrica} dell'autovalore $ \lambda $.
				La $ i-$esima catena ha lunghezza $ p_i $ e termina con un autovettore $ \vec{v}_1^{(i)} $. Gli altri autovettore della catena sono autovettori generalizzati ma non sono autovettori. Poiché in totale gli autovettori generalizzati sono $ \nu $ vale anche 
				\[
					\sum_{i=1}^{\mu} p_i = \nu
				\]
				La lunghezza della catena più lunga $ \pi = \max \{p_1,p_2,\dots,p_{\mu}\} $ è detta \textbf{indice} dell'autovalore $ \lambda $.
 			\end{defn}
			
\end{document}